{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collectible-jonathan",
   "metadata": {},
   "source": [
    "##### Copyright 2021 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-dealer",
   "metadata": {},
   "source": [
    "# Use GPUs with Apache Beam\n",
    "\n",
    "This notebook uses the [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method) to calculate pi (3.14159...) and demonstrate the performance difference at the same scale of sample size between different runtimes:\n",
    "\n",
    "- Native Python code\n",
    "- Jitted machine code\n",
    "- On GPU (CUDA)\n",
    "- Beam pipeline locally on GPU\n",
    "- Beam pipeline on Dataflow with GPU\n",
    "\n",
    "**Note**: this notebook does not work if your notebook instance does not have a GPU or the drivers are not installed. If you haven't done so, create a [new Dataflow Notebooks](https://pantheon.corp.google.com/dataflow/notebooks/list/instances) instance `With 1 NVIDIA Tesla T4` and check the option to `Install NVIDIA GPU driver automatically for me`.\n",
    "\n",
    "More details can be found on [Dataflow support for GPUs](https://cloud.google.com/dataflow/docs/concepts/gpu-support) if you want to productionize Beam pipelines on Dataflow with GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-control",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "We will use [numba](https://numba.pydata.org/) to compile code in this notebook for different runtimes.\n",
    "\n",
    ">Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n",
    "\n",
    "It also supports GPU acceleration.\n",
    "\n",
    "**Note**: you might need to restart the kernel if this is the first time you've installed the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-checklist",
   "metadata": {},
   "source": [
    "Let's check if the CUDA libraries are available (*if not, your notebook instance probably wasn't created with a GPU*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find /usr/local/cuda-* -iname 'libdevice'\n",
    "!find /usr/local/cuda-* -iname 'libnvvm.so'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-documentary",
   "metadata": {},
   "source": [
    "Let's disable non-error logs for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-ministry",
   "metadata": {},
   "source": [
    "## Native Python & jitted machine code\n",
    "\n",
    "Below we have defined two functions with exactly the same code:\n",
    "\n",
    "- `python_cpu_monte_carlo_pi` is a plain native Python function that runs on the CPU.\n",
    "- `machine_code_cpu_monte_carlo_pi` has an annotation `@jit(nopython=True)` that compiles the Python code into machine code that runs on the CPU too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "def python_cpu_monte_carlo_pi(sample_size):\n",
    "    acc = 0\n",
    "    for i in range(sample_size):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x * x + y * y) <= 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / sample_size\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def machine_code_cpu_monte_carlo_pi(sample_size):\n",
    "    acc = 0\n",
    "    for i in range(sample_size):\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        if (x * x + y * y) <= 1.0:\n",
    "            acc += 1\n",
    "    return 4.0 * acc / sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-format",
   "metadata": {},
   "source": [
    "Let's choose a sample size (100,000,000) as a constant computation complexity between both runs.\n",
    "\n",
    "The most expensive yet parallelizable part of the computation is generating the random numbers. Neither function optimizes that part though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-warrant",
   "metadata": {},
   "source": [
    "### Performance of native Python code\n",
    "\n",
    "It should take ~40 seconds for native Python code to calculate pi with a sample size of 100,000,000.\n",
    "\n",
    "**Note**: The performance might vary from run to run. It might also vary between different notebook instances if they are configured differently. The same applies to other runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "pi = python_cpu_monte_carlo_pi(sample_size)\n",
    "dt = timer() - start\n",
    "print(f'Monte Carlo pi calculated as {pi} in {dt} s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-collective",
   "metadata": {},
   "source": [
    "### Performance of jitted machine code\n",
    "\n",
    "It should only take a little over 1 second for jitted (the first time execution is a bit slower: ~2 seconds, because the source code is not jitted yet) machine code to calculate pi with a sample size of 100,000,000.\n",
    "\n",
    "After jitting the Python code, let's use `%timeit` to run the code multiple times to measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run is slower because the code is not jitted.\n",
    "start = timer()\n",
    "pi = machine_code_cpu_monte_carlo_pi(sample_size)\n",
    "dt = timer() - start\n",
    "print(f'Monte Carlo pi calculated as {pi} in {dt} s.')\n",
    "\n",
    "# Run the jitted code multiple times to measure performance.\n",
    "%timeit machine_code_cpu_monte_carlo_pi(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-dimension",
   "metadata": {},
   "source": [
    "## On GPU (CUDA)\n",
    "\n",
    "In the below example, we have rearranged the native Python function into two parts:\n",
    "\n",
    "- The first part, `gpu_monte_carlo_pi_sampler`, which generates random points and aggregate counts for a subset of the target sample size, is executed on the GPU.\n",
    "- The second part, `calculate_pi`, which aggregates value from all sub sample sets and calculates pi, is compiled into machine code and executed on the GPU.\n",
    "\n",
    "**Note**:`njit` is similar to `jit` but for `numpy`.\n",
    "\n",
    "The example code uses only 1 grid, 80 blocks and 64 threads per block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, njit\n",
    "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_monte_carlo_pi_sampler(rng_states, sub_sample_size, acc):\n",
    "    \"\"\"Uses GPU to sample random values and accumulates the sub count\n",
    "    of values within the circle.\n",
    "    \"\"\"\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < acc.shape[0]:\n",
    "        sub_acc = 0\n",
    "        for i in range(sub_sample_size):\n",
    "            x = xoroshiro128p_uniform_float32(rng_states, pos)\n",
    "            y = xoroshiro128p_uniform_float32(rng_states, pos)\n",
    "            if (x * x + y * y) <= 1.0:\n",
    "                sub_acc += 1\n",
    "        acc[pos] = sub_acc\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def calculate_pi(acc, sample_size):\n",
    "    \"\"\"Uses machine code on CPU to aggregate and calculate pi since there\n",
    "    is less parallelism here.\n",
    "    \"\"\"\n",
    "    return 4 * np.sum(acc) / sample_size\n",
    "\n",
    "threadsperblock = 64\n",
    "blocks = 80\n",
    "# An 1-d array on host to hold accumulated sub count of points in the circle.\n",
    "h_acc = np.zeros(threadsperblock * blocks, dtype=np.float32)\n",
    "# Copy the numpy array from host (CPU) to device (GPU) to avoid back-and-forth copies.\n",
    "d_acc = cuda.to_device(h_acc)\n",
    "rng_states = create_xoroshiro128p_states(threadsperblock * blocks, seed=1)\n",
    "# Each thread should only generate sub_sample_size of random points.\n",
    "sub_sample_size = sample_size // d_acc.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-gamma",
   "metadata": {},
   "source": [
    "### Performance of on GPU (CUDA)\n",
    "\n",
    "It should take ~5ms (the first time execution is a bit slower: ~0.5 seconds, because the source code is not jitted yet) to calculate pi on a GPU using above configuration with a sample size of 100,000,000.\n",
    "\n",
    "It's **8000 times faster** than native Python on CPU and **200 times** faster than machine code on CPU. And we haven't even pushed the single GPU to its limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run is slower because the code is not jitted.\n",
    "start=timer()\n",
    "gpu_monte_carlo_pi_sampler[blocks, threadsperblock](rng_states, sub_sample_size, d_acc)\n",
    "# Copy the array on device back to host.\n",
    "d_acc.copy_to_host(h_acc)\n",
    "pi = calculate_pi(h_acc, sample_size)\n",
    "dt = timer() - start\n",
    "print(f'Monte Carlo pi calculated as {pi} in {dt} s.')\n",
    "\n",
    "# Run the jitted code multiple times to measure performance.\n",
    "%timeit gpu_monte_carlo_pi_sampler[blocks, threadsperblock](rng_states, sub_sample_size, d_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-triangle",
   "metadata": {},
   "source": [
    "## Running a Beam pipeline locally on GPU\n",
    "\n",
    "It might not be obvious why you need to wrap your code that can already run on a GPU\n",
    "into an Apache Beam pipeline.\n",
    "\n",
    "The advantage of Beam with GPU is that you can later run the pipeline on Dataflow or any other runner/clusters\n",
    "so that you can distribute/scale the work to as many GPUs as you want.\n",
    "\n",
    "Note: you might need to configure worker machines so that they have GPU devices and drivers available.\n",
    "For example, [using GPUs with Dataflow](https://cloud.google.com/dataflow/docs/guides/using-gpus).\n",
    "You also need to constrain your GPU usages to work within the hardware limit. See\n",
    "[GPUs and worker parallelism](https://cloud.google.com/dataflow/docs/concepts/gpu-support#gpus_and_worker_parallelism).\n",
    "\n",
    "In the below example, we build a Beam pipeline with code similar to the plain [on-GPU example](#On-GPU-(CUDA)) and run the pipeline locally on this notebook instance.\n",
    "\n",
    "First, we create a pipeline instance with options that utilize all CPU cores to schedule work when running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "from apache_beam.runners.interactive import interactive_beam as ib\n",
    "\n",
    "\n",
    "options = pipeline_options.PipelineOptions(\n",
    "    direct_num_workers=0,  # default threads/subprocess to the number of cores on this machine\n",
    "    direct_running_mode='multi_threading')\n",
    "p = beam.Pipeline(InteractiveRunner(), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-swiss",
   "metadata": {},
   "source": [
    "Then, we define a `DoFn` as a `Sampler` that uses the GPU to process elements.\n",
    "\n",
    "Each element is a tuple of 2 `int` values:\n",
    "\n",
    "- first value: the seed of a random number generator\n",
    "- second value: the size of sample values to be generated\n",
    "\n",
    "The `DoFn` itself runs as native Python code on the CPU while the inner logic of `gpu_monte_carlo_pi_sampler` runs on GPU.\n",
    "\n",
    "You can also see that to avoid back-and-forth copying of the result array, `cuda.to_device()` copies the array to GPU, then `copy_to_host` returns the array on GPU back to the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(beam.DoFn):\n",
    "    def __init__(self, blocks=80, threads_per_block=64):\n",
    "        # Uses only 1 cuda grid with below config.\n",
    "        self.blocks = blocks\n",
    "        self.threads_per_block = threads_per_block\n",
    "    \n",
    "    def setup(self):\n",
    "        import numpy as np\n",
    "        # An array on host as the prototype of arrays on GPU to\n",
    "        # hold accumulated sub count of points in the circle.\n",
    "        self.h_acc = np.zeros(\n",
    "            self.threads_per_block * self.blocks, dtype=np.float32)\n",
    "\n",
    "    def process(self, element: Tuple[int, int]):\n",
    "        from numba import cuda\n",
    "        from numba.cuda.random import create_xoroshiro128p_states\n",
    "        from numba.cuda.random import xoroshiro128p_uniform_float32\n",
    "        \n",
    "        @cuda.jit\n",
    "        def gpu_monte_carlo_pi_sampler(rng_states, sub_sample_size, acc):\n",
    "            \"\"\"Uses GPU to sample random values and accumulates the sub count\n",
    "            of values within a circle of radius 1.\n",
    "            \"\"\"\n",
    "            pos = cuda.grid(1)\n",
    "            if pos < acc.shape[0]:\n",
    "                sub_acc = 0\n",
    "                for i in range(sub_sample_size):\n",
    "                    x = xoroshiro128p_uniform_float32(rng_states, pos)\n",
    "                    y = xoroshiro128p_uniform_float32(rng_states, pos)\n",
    "                    if (x * x + y * y) <= 1.0:\n",
    "                        sub_acc += 1\n",
    "                acc[pos] = sub_acc\n",
    "\n",
    "        rng_seed, sample_size = element\n",
    "        d_acc = cuda.to_device(self.h_acc)\n",
    "        sample_size_per_thread = sample_size // self.h_acc.shape[0]\n",
    "        rng_states = create_xoroshiro128p_states(self.h_acc.shape[0], seed=rng_seed)\n",
    "        gpu_monte_carlo_pi_sampler[self.blocks, self.threads_per_block](\n",
    "            rng_states, sample_size_per_thread, d_acc)\n",
    "        yield d_acc.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-republic",
   "metadata": {},
   "source": [
    "We can divide the work to 100 samplers. In a distributed environment, each sampler can run on a different machine with its own GPU(s).\n",
    "\n",
    "Here for simplicity, we run the pipeline locally and all samplers would share the same GPU on this notebook instance.\n",
    "\n",
    "We start the pipeline by creating a PCollection of 100 tuples of random number seeds (from 0 to 99) and sample size per sampler (1,000,000).\n",
    "\n",
    "Then we let the `Sampler DoFn` take these tuples as inputs to generate sample values.\n",
    "\n",
    "Each sampler has `threads_per_block` * `blocks` = **5120 threads** running in parallel\n",
    "**on the GPU**. \n",
    "\n",
    "And we have **100 samplers** running concurrently scheduled by Beam **on all CPU cores** on the machine running this notebook.\n",
    "\n",
    "In the collected data, you can see the aggregated counts of \"random points within the circle\" for each GPU thread by each sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_count = 100\n",
    "sample_size_per_sampler = sample_size // sampler_count\n",
    "\n",
    "samplers_per_gpu_thread = (p \n",
    "                           | beam.Create([(i, sample_size_per_sampler) for i in range(sampler_count)])\n",
    "                           | beam.ParDo(Sampler()))\n",
    "\n",
    "ib.collect(samplers_per_gpu_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-objective",
   "metadata": {},
   "source": [
    "Everything below runs as native Python code on the CPU.\n",
    "\n",
    "To calculate pi, we need to aggregate all values from all GPU threads for each sampler.\n",
    "\n",
    "**Note**: we use numpy to sum the values (`np.float32`) and then convert them back to the native Python type (`int`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_sum(x):\n",
    "    import numpy as np\n",
    "    return np.sum(x).astype(int).item()\n",
    "\n",
    "acc_per_sampler = samplers_per_gpu_thread | beam.Map(np_sum).with_output_types(int)\n",
    "\n",
    "# Sum up per-gpu-thread count of values falling into the circle of Monte Carlo pi calculation for each sampler.\n",
    "ib.show(acc_per_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-subdivision",
   "metadata": {},
   "source": [
    "Then we combine all values from all samplers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_acc = acc_per_sampler | beam.CombineGlobally(sum)\n",
    "\n",
    "# Sum up the count of values falling into the circle of Monte Carlo pi calculation from all samplers.\n",
    "ib.show(sum_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-heather",
   "metadata": {},
   "source": [
    "Once we have all values aggregated, we can use the formula to calculate and print pi.\n",
    "\n",
    "The visualization shows the pipeline graph. Let's make sure it's correctly constructed and doesn't have corrupted states from notebook executions.\n",
    "\n",
    "**Note**: if the graph is mixed with transforms that are applied by out-of-order execution and re-execution of notebooks, please re-execute the code\n",
    "from where the pipeline is created or restart the kernel and re-execute the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculatePi(beam.DoFn):\n",
    "    def __init__(self, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "    \n",
    "    def process(self, count: int):\n",
    "        yield 4 * count / self.sample_size\n",
    "\n",
    "calculated_pi = sum_acc | beam.ParDo(CalculatePi(sample_size))\n",
    "            \n",
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-ethiopia",
   "metadata": {},
   "source": [
    "### Performance of Beam pipeline locally on GPU\n",
    "\n",
    "It should take a few seconds to calculate pi using Beam on GPU with a sample size of 100,000,000.\n",
    "\n",
    "Though it's not as fast as GPU + machine code executed on a single machine, it provides the scalability to run the code\n",
    "on distributed systems, and the performance is almost on par with pure machine code on a single machine. You can also improve the\n",
    "performance further by compiling those transforms written in native Python code into machine code with jit/njit.\n",
    "\n",
    "The example has demonstrated how to write a Beam pipeline using a GPU, the performance increment when using a GPU compared to native Python code on CPU, and the small overhead of a Beam pipeline on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.recordings.clear(p)\n",
    "start=timer()\n",
    "print(f'Monte Carlo pi calculated as: {ib.collect(calculated_pi)[0][0]} in {timer() - start} s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236e010-4e7a-431f-b0e3-bba9c58b5870",
   "metadata": {},
   "source": [
    "## Running a Beam pipeline on Dataflow with GPU\n",
    "\n",
    "Normally, you can [manage Python pipeline dependencies](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies) with some extra pipeline options such as `requirements_file`, `extra_package` and `setup_file`. However, you have to create a [custom container](https://cloud.google.com/dataflow/docs/guides/using-custom-containers) specifically [configured](https://cloud.google.com/dataflow/docs/guides/using-gpus#building_a_custom_container_image) to instruct Dataflow to execute your pipeline on GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d66903e-9b30-48ca-bca9-a382987c7900",
   "metadata": {},
   "source": [
    "From this notebook, you can create a custom container that supports [CUDA](https://developer.nvidia.com/cuda-toolkit) and use it to run the pipeline on Dataflow with GPU.\n",
    "\n",
    "- The Monte Carlo pi example requires additional dependencies: `numba` and `numpy` from PYPI.\n",
    "- You can use `nvidia-tesla-t4` GPUs when running the pipeline on Dataflow.\n",
    "- You can select a Beam SDK version. The example uses 2.33.0.\n",
    "- The example uses Python 3.7. This should match the version used in the custom container. Check the Python version in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d0c5d-a6ba-4fa6-850e-797060e6e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b40834-99f1-495f-b230-71005c45eda8",
   "metadata": {},
   "source": [
    "Make sure the service account running this notebook instance has minimum [permissions and roles](https://cloud.google.com/container-registry/docs/access-control#permissions_and_roles) to build and publish container images to the current project's container registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c7f83-ebb0-4d81-bf13-35998c0b8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gcloud configuration\n",
    "!gcloud iam service-accounts describe $(gcloud config get-value account)\n",
    "!gcloud projects get-iam-policy $(gcloud config get-value project)  \\\n",
    "  --flatten=\"bindings[].members\" \\\n",
    "  --format='table(bindings.role)' \\\n",
    "  --filter=\"bindings.members:$(gcloud config get-value account)\"\n",
    "\n",
    "# Enable the container registry service\n",
    "!gcloud services enable containerregistry.googleapis.com\n",
    "\n",
    "# Configure docker\n",
    "!yes | gcloud auth configure-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab01e5d6-6df7-4fd0-a8e1-5e25bcb76bba",
   "metadata": {},
   "source": [
    "Create a temporary directory and a dockerfile as the context to build a container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0afd5-91ce-467b-9781-147110a80b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /home/jupyter/.cc_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ca586-6aff-4647-a6ba-ba07a7336ec5",
   "metadata": {},
   "source": [
    "The dockerfile follows this [example](https://cloud.google.com/dataflow/docs/guides/using-gpus#installing_a_specific_python_version) with a little change based on the SDK version and Python version. It uses the `devel` versioned image so that `nvmm` is available to compile the `numba.cuda.jit` code. Make sure the CUDA version from the [base image from NVIDIA](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda/tags) matches the installed version on VMs used by Dataflow. If something went wrong, refer [Debug with a standalone VM](https://cloud.google.com/dataflow/docs/guides/using-gpus#debug_with_a_standalone_vm) to check the CUDA version using the command `nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c4cba-1ecd-4adc-8872-c5df5ef6827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/jupyter/.cc_gpu/Dockerfile\n",
    "FROM nvcr.io/nvidia/cuda:11.0.3-devel-ubuntu20.04\n",
    "\n",
    "# Avoid tzdata hanging the docker build.\n",
    "ARG DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "RUN \\\n",
    "    apt update \\\n",
    "    && apt install -y software-properties-common \\\n",
    "    && add-apt-repository ppa:deadsnakes/ppa \\\n",
    "    && apt remove -y software-properties-common \\\n",
    "    && apt autoremove -y \\\n",
    "    && apt update \\\n",
    "    && apt install -y curl python3.7 python3-distutils \\\n",
    "    && update-alternatives --install /usr/bin/python python /usr/bin/python3.7 10 \\\n",
    "    && curl https://bootstrap.pypa.io/get-pip.py | python \\\n",
    "    && python -m pip install --upgrade pip \\\n",
    "    && pip install --no-cache-dir \"apache-beam[gcp]==2.33.0\" numba numpy pandas\\\n",
    "    && pip check\n",
    "\n",
    "# Copy the Apache Beam worker dependencies from the Beam Python 3.7 SDK image.\n",
    "COPY --from=apache/beam_python3.7_sdk:2.33.0 /opt/apache/beam /opt/apache/beam\n",
    "\n",
    "# Set the entrypoint to Apache Beam SDK worker launcher.\n",
    "ENTRYPOINT [ \"/opt/apache/beam/boot\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd72524-9b30-4f96-9008-fd77a217af80",
   "metadata": {},
   "source": [
    "Use [Cloud Build](https://cloud.google.com/build) to build the container image and publish it to a container registry that your Dataflow service has access to. The example publishes it to the currently configured project's container registry. Change the command below accordingly if you have another publish destination in mind.\n",
    "\n",
    "**Note**: Building and publishing a fresh container image takes a relatively long time. You don't have to re-execute it if there is no change to the dockerfile.\n",
    "\n",
    "**Warning**: This notebook instance comes with docker (from its host VM) to support xLang transforms such as [Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/), **do not** use it to build docker containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40df41-f36f-4d1a-8749-a3ba703e5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -e\n",
    "project=$(gcloud config get-value project)\n",
    "cd /home/jupyter/.cc_gpu\n",
    "gcloud builds submit --tag gcr.io/$project/cc_gpu:latest --timeout=20m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562561f-e2a9-479a-8c9c-cd2923eb497b",
   "metadata": {},
   "source": [
    "The container image `gcr.io/$project/cc_gpu:latest` should be available to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59025d2-d64f-493b-b110-167ca2707a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud container images list | grep cc_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba28312-6820-476a-a33e-0250796d7a66",
   "metadata": {},
   "source": [
    "Configure the pipeline options and add additional transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94fdce-c1fe-4527-833c-b617704b93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import DebugOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "import google.auth\n",
    "\n",
    "\n",
    "_, PROJECT = google.auth.default()\n",
    "GCS_BUCKET = 'gs://your-GCS-bucket'\n",
    "\n",
    "# Write the result to a file on Cloud Storage\n",
    "_ = calculated_pi | beam.io.WriteToText(f'{GCS_BUCKET}/staging/calculated_pi')\n",
    "\n",
    "options = pipeline_options.PipelineOptions(\n",
    "  project=PROJECT,\n",
    "  region='us-central1',\n",
    "  staging_location=f'{GCS_BUCKET}/staging',\n",
    "  temp_location=f'{GCS_BUCKET}/temp',\n",
    "  sdk_container_image=f'gcr.io/{PROJECT}/cc_gpu:latest',\n",
    "  machine_type='n1-standard-4',\n",
    "  disk_size_gb=50)\n",
    "options.view_as(DebugOptions).add_experiment('worker_accelerator=type:nvidia-tesla-t4;count:1;install-nvidia-driver')\n",
    "options.view_as(DebugOptions).add_experiment('use_runner_v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad28ce3-8b39-4184-b816-3fd2f0094399",
   "metadata": {},
   "source": [
    "Now run the pipeline on Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f942f8-90b0-4218-80a5-e420cbbe6263",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (pipeline_result._job.location, pipeline_result._job.id, pipeline_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))\n",
    "\n",
    "pipeline_result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfa5c81-1d09-4b0d-899d-eb4f27a42edd",
   "metadata": {},
   "source": [
    "Check the result once the job is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3dc23b-543a-4972-8a7f-e899c5cc7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {GCS_BUCKET}/staging/calculated_pi*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
