{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Streaming Word Count\n",
    "\n",
    "This example demonstrates how to set up a streaming processing pipeline that reads from a\n",
    "[Google Pub/Sub](https://cloud.google.com/pubsub) topic. Each message in the Pub/Sub topic is a word from Shakespeare's work *King Lear*, \n",
    "\n",
    "The difference between this example and [Example 1](01-Word_Count.ipynb) is that Example 1\n",
    "takes in a **bounded** source as an input, while this example takes in\n",
    "an *infinite* data stream, or an **unbounded** source, that is constantly providing data in real time.\n",
    "\n",
    "The pipeline performs a frequency count on each of those words by window.\n",
    "\n",
    "You can use this notebook to explore the data in each `PCollection`.\n",
    "\n",
    "Note that running this example may incur a small [charge](https://cloud.google.com/pubsub/pricing#message_delivery_pricing) if your aggregated Pub/Sub usage is past the free tier.\n",
    "\n",
    "\n",
    "Check your gcloud configuration. If not specified, the account used is the default compute engine service account that looks like `${project_number}-compute@developer.gserviceaccount.com`.\n",
    "\n",
    "Check the [IAM](https://cloud.google.com/iam) configuration of the account to **ensure it has authorization** to run the code in your notebooks. In this example, to read from a Pub/Sub topic, it needs at least `Pub/Sub Editor` role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the Pub/Sub API is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services list | grep -q pubsub && echo \"Enabled\" || echo \"Not enabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the pubsub service is not enabled and the above account in use has project `Editor` role, uncomment and execute below command to\n",
    "[allow](https://cloud.google.com/apis/docs/getting-started#enabling_apis) your project to access the Pub/Sub service.\n",
    "              \n",
    "Otherwise, please login as or ask the project admin to enable Pub/Sub service in its console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud services enable pubsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive import interactive_runner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are setting up the options to create the streaming pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions()\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(pipeline_options.StandardOptions).streaming = True\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "# The project will be used for creating a subscription to the Pub/Sub topic.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline reads from Google Cloud Pub/Sub, which is an unbounded source. By default, *Apache Beam Notebooks* will record\n",
    "data from the unbounded sources for replayability. \n",
    "\n",
    "In this example, the pipeline reads from a public Pub/Sub topic `projects/pubsub-public-data/topics/shakespeare-kinglear` that outputs multiple words from *King Lear* every second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Google Cloud PubSub topic for this example.\n",
    "topic = \"projects/pubsub-public-data/topics/shakespeare-kinglear\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets how long the Interactive Runner records data from each unbounded source. These recordings are used to enable a deterministic replay of the entire pipeline. The following sets the data recording duration to 2 minutes (120 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.options.recording_duration = '2m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a pipeline with the *Interactive Runner* as the runner with the options we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(interactive_runner.InteractiveRunner(), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a `PTransform` that will create a subscription to the given Pub/Sub topic and reads from the subscription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = p | \"read\" >> beam.io.ReadFromPubSub(topic=topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are reading from an unbounded source, we need to create a windowing scheme so that we can\n",
    "count the words by window. The following creates fixed windowing with each window being 10 seconds in duration.\n",
    "For more information about windowing in Apache Beam, visit the [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/#windowing-basics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_words = (words \n",
    "                  | \"window\" >> beam.WindowInto(beam.window.FixedWindows(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `PTransform` will count the words by window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_word_counts = (windowed_words\n",
    "                        | \"count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ib.show()` method takes a `PCollection` as a parameter, runs the pipeline that contributes to it, and\n",
    "shows its content as data comes in. This method will return when all the data has been read.\n",
    "\n",
    "The optional parameter `include_window_info=True` will include the window information for each element in the output.\n",
    "You will see 3 additional columns: `event_time`, `windows`, and `pane_info`.\n",
    "`event_time` is the timestamp associated with the value.\n",
    "`windows` in this example tells you the start timestamp of the window and its duration.\n",
    "`pane_info` describes the [triggering](https://beam.apache.org/documentation/programming-guide/#triggers) information for the pane that contained the value.\n",
    "\n",
    "This example does not use custom triggering so by default there will be only one pane per window labeled `Pane 0`.\n",
    "\n",
    "Note that this also automatically records a bounded segment of the unbounded source until the 2-minute recording duration passes. To stop the `ib.show()` early, you can click the button with tooltip `Interrupt the kernel` in the toolbar (see [FAQ #5. Why does the `ib.collect` or `ib.show` take forever to finish execution?\n",
    "    How do I stop it?](../../faq.md#q5))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(windowed_word_counts, include_window_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can provide 2 more options to limit how much of the recorded data to show.\n",
    "\n",
    "The optional parameter `n=20` limits the `ib.show()` to show at most 20 elements. If not set, the default value `inf` tails all elements until the source recording is over.\n",
    "\n",
    "The optional parameter `duration=30` limits the `ib.show()` to show at most elements that are computed based on the first 30 seconds worth of data from the recorded sources. If not set, the default value `inf` tails all elements until the source recording is over.\n",
    "\n",
    "If both parameters are set, the `ib.show()` stops whenever either threshold is met. So below `ib.show()` shows at most 20 elements that are computed based on the first 30 seconds worth of data from the recorded sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(windowed_word_counts, include_window_info=True, n=20, duration=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have recorded a bounded segment of the unbounded source, the following will show the same data\n",
    "as the previous `ib.show()` call. This is to ensure replayability so that you can iteratively augment\n",
    "your pipeline and verify the output with the same input, which you will see in future cells in this notebook.\n",
    "Note the parameter `visualize_data=True`. This optional parameter gives you a visualization of the data (see [FAQ #3.How do I read the visualization](../../faq.md#q3)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(windowed_word_counts, include_window_info=True, visualize_data=True, n=20, duration=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, to ensure replayability for iterative prototyping of your pipeline,\n",
    "`ib.show()` calls will reuse the recorded data by default. You can change this behavior and\n",
    "have it always fetch new data, by doing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this only if you would like to change the replay behavior:\n",
    "# ib.options.enable_recording_replay = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `PTransform` will convert the words to lowercase and then count them by window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_lower_word_counts = (windowed_words\n",
    "                              | beam.Map(lambda word: word.lower())\n",
    "                              | \"count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have not changed `ib.options.enable_recording_replay`, the following will return the count using the same words \n",
    "as before but with lowercase.\n",
    "Because all words are converted to lowercase before being counted, some words will have a higher count than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(windowed_lower_word_counts, include_window_info=True, n=20, duration=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives you a [Pandas Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that represents the `PCollection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.collect(windowed_lower_word_counts, include_window_info=True, n=20, duration=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use optional parameters `n` and `duration` in `ib.show()` or `ib.collect()` to control how much of the recorded data to operate on for a streaming pipeline to iterate your pipeline development faster. However, if you want to stop a recording early on or record fresh data, you can use `ib.recordings` APIs to explicitly control the long running background source recording jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The describe method tells you about the current status of a background source recording job for a given streaming pipeline.\n",
    "# If it's in a cancelled state, it means the background source recording job already hits one of the hard caps configured in ib.options.\n",
    "ib.recordings.describe(p)\n",
    "# The stop method stops the background source recording job immediately.\n",
    "ib.recordings.stop(p)\n",
    "# The clear method clears the recorded data.\n",
    "ib.recordings.clear(p)\n",
    "# The record method explicitly starts a new background source recording job for the given streaming pipeline.\n",
    "# If `clear` is called while `record` is not called, the next `ib.show()` or `ib.collect()` starts a new recording implicitly.\n",
    "# ib.recordings.record(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above `clear` and optional explicit `record`, below `ib.collect` generates different output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.collect(windowed_lower_word_counts, include_window_info=True, n=20, duration=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done with this example, you might want to visit the [PubSub subscription page](https://console.cloud.google.com/cloudpubsub/subscription/list) to delete any subscription created by this example.\n",
    "\n",
    "Just like the first example, this example is designed to run easily on a single machine. If the input stream has a very high volume, add an output sink to your `PCollection` result by doing something like:\n",
    "```\n",
    "windowed_lower_word_counts | beam.io.<some output transform>\n",
    "```\n",
    "and let [Google Cloud Dataflow](https://cloud.google.com/dataflow) run your pipeline.\n",
    "\n",
    "You can find the list of built-in input and output transforms [here](https://beam.apache.org/documentation/io/built-in/).\n",
    "\n",
    "Refer to the [user guide](https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development) on how to run a Dataflow job using a pipeline assembled from your notebook. You can also refer to [this walkthrough](Dataflow_Word_Count.ipynb) which is based on the [first word count example notebook](01-Word_Count.ipynb).\n",
    "\n",
    "If you have any feedback on this notebook, drop us a line at beam-notebooks-feedback@google.com."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
