{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run the examples on Dataflow\n",
    "\n",
    "This notebook illustrates how to run the pipeline in the [First Word Count example](01-Word_Count.ipynb) with the Dataflow Runner, instead of the Interactive Runner.\n",
    "\n",
    "Note that running this example incurs a small [charge](https://cloud.google.com/dataflow/pricing) from Dataflow.\n",
    "\n",
    "Let's make sure the Dataflow API is enabled. This [allows](https://cloud.google.com/apis/docs/getting-started#enabling_apis) your project to access the Dataflow service:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the necessary imports:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import apache_beam as beam\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "\n",
    "\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell was copied from the [01-Word_Count](01-Word_Count.ipynb) example. It contains the same pipeline construction code that performs a word count on text files hosted on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWordsFromText(beam.PTransform):\n",
    "    \n",
    "    def __init__(self, file_pattern):\n",
    "        self._file_pattern = file_pattern\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll.pipeline\n",
    "                | beam.io.ReadFromText(self._file_pattern)\n",
    "                | beam.FlatMap(lambda line: re.findall(r'[\\w\\']+', line.strip(), re.UNICODE)))\n",
    "    \n",
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "words = p | 'read' >> ReadWordsFromText('gs://apache-beam-samples/shakespeare/kinglear.txt')\n",
    "\n",
    "counts = (words \n",
    "          | 'count' >> beam.combiners.Count.PerElement())\n",
    "\n",
    "lower_counts = (words\n",
    "                | \"lower\" >> beam.Map(lambda word: word.lower())\n",
    "                | \"lower_count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `Pipeline` is constructed by an `InteractiveRunner`, so you can use operations such as `ib.collect` or `ib.show`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataflow Additions\n",
    "\n",
    "Now, for something a bit different. Because Dataflow executes in the cloud, you need to output to a cloud sink. In this case, you are loading the transformed data into Cloud Storage.\n",
    "\n",
    "First, set up the `PipelineOptions` to specify to the Dataflow service the Google Cloud project, the region to run the Dataflow Job, and the SDK location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Apache Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions(flags=[])\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Sets the Google Cloud Region in which Cloud Dataflow runs.\n",
    "options.view_as(GoogleCloudOptions).region = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Adjust the following to choose a Cloud Storage location.\n",
    "dataflow_gcs_location = 'gs://<CHANGE ME>/dataflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataflow Staging Location. This location is used to stage the Dataflow Pipeline and SDK binary.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Dataflow Temp Location. This location is used to store temporary files or intermediate results before finally outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory to store the output files of the job.\n",
    "output_gcs_location = '%s/output' % dataflow_gcs_location\n",
    "\n",
    "# Specifying the Cloud Storage location to write `counts` to,\n",
    "# based on the `output_gcs_location` variable set earlier.\n",
    "(counts | 'Write counts to Cloud Storage' \n",
    " >> beam.io.WriteToText(output_gcs_location + '/wordcount-output.txt'))\n",
    "\n",
    "# Specifying the Cloud Storage location to write `lower_counts` to,\n",
    "# based on the `output_gcs_location` variable set earlier.\n",
    "(lower_counts | 'Write lower counts to Cloud Storage' \n",
    " >> beam.io.WriteToText(output_gcs_location + '/wordcount-lower-output.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT! Ensure that the graph is correct before sending it out to Dataflow.\n",
    "# Because this is a notebook environment, unintended additions to the graph may have occurred when rerunning cells. \n",
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pipeline\n",
    "\n",
    "Now you are ready to run the pipeline on Dataflow. `run_pipeline()` runs the pipeline and return a pipeline result object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result = DataflowRunner().run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `pipeline_result` handle, the following code builds a link to the Google Cloud Console web page that shows you details of the Dataflow job you just started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "url = ('https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s' % \n",
    "      (pipeline_result._job.location, pipeline_result._job.id, pipeline_result._job.projectId))\n",
    "display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the job to finish. The following call blocks notebook execution until the Dataflow job is finished. This will take a few minutes. ",
    "With this example, you will notice the Dataflow job takes a much longer time to finish compared to running directly in the notebook environment.\n",
    "This is because it takes time for a Dataflow job to set up the environment for parallel execution. Note that this is a very small job, and Dataflow is more suited for parallelizing and running large jobs\n",
    "(e.g. thousands of files with billions of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result.wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the results\n",
    "\n",
    "Now that the job is finished, check the results in Cloud Storage using the [`gsutil`](https://cloud.google.com/storage/docs/gsutil) command-line tool. Note that `beam.io.WriteToText` writes the results in a sharded set of output files. For example, if the output is specified as `gs://my_bucket/output_directory/result.txt`, the results are written in files with names like `gs://my_bucket/output_directory/result.txt-<shard>-of-<number-of-shards>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {output_gcs_location}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the content of the files by looking at the first 10 lines of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {output_gcs_location}/wordcount-output.txt* | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat {output_gcs_location}/wordcount-lower-output.txt* | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Using this technique, you can also try launching Dataflow jobs for other examples listed.\n",
    "\n",
    "If you have any feedback on this notebook, drop us a line at beam-notebooks-feedback@google.com."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
