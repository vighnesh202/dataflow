{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Streaming NYC Taxi Ride Data\n",
    "\n",
    "This example demonstrates how to set up a streaming pipeline that processes a stream that contains NYC taxi ride data.\n",
    "Each element in the stream contains the location of the taxi, the timestamp, the meter reading, the meter increment, the passenger count, and ride status in JSON format.\n",
    "\n",
    "You'll be able to use this notebook to explore the data in each `PCollection`.\n",
    "\n",
    "Note that running this example may incur a small [charge](https://cloud.google.com/pubsub/pricing#message_delivery_pricing) if your aggregated Pub/Sub usage is past the free tier.\n",
    "\n",
    "Let's make sure the Pub/Sub API is enabled. This [allows](https://cloud.google.com/apis/docs/getting-started#enabling_apis) your project to access the Pub/Sub service:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable pubsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive import interactive_runner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.transforms import trigger\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "import google.auth\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# The Google Cloud PubSub topic that we are reading from for this example.\n",
    "topic = \"projects/pubsub-public-data/topics/taxirides-realtime\"\n",
    "\n",
    "# So that Pandas Dataframes do not truncate data...\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are setting up the options to create the streaming pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Beam pipeline options.\n",
    "options = pipeline_options.PipelineOptions()\n",
    "\n",
    "# Sets the pipeline mode to streaming, so we can stream the data from PubSub.\n",
    "options.view_as(pipeline_options.StandardOptions).streaming = True\n",
    "\n",
    "# Sets the project to the default project in your current Google Cloud environment.\n",
    "# The project will be used for creating a subscription to the PubSub topic.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with unbounded sources. By default, *Apache Beam Notebooks* will record data from the unbounded sources for replayability. \n",
    "The following sets the data recording duration to 2 minutes (120 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.options.recording_duration = '2m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a pipeline with the *Interactive Runner* as the runner with the options we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(interactive_runner.InteractiveRunner(), options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a `PTransform` that will create a subscription to the given Pub/Sub topic and reads from the subscription. \n",
    "The data is in JSON format, so we add another `Map` `PTransform` to parse the data as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = p | \"read\" >> beam.io.ReadFromPubSub(topic=topic) | beam.Map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can inspect the raw JSON data by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this if you want to inspect the raw JSON data:\n",
    "# ib.show(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are reading from an unbounded source, we need to create a windowing scheme.\n",
    "Let's do sliding windows with a 10-second duration each window, with one second for each slide.\n",
    "For more information about windowing in Apache Beam, visit the [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/#windowing-basics).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_data = (data | \"window\" >> beam.WindowInto(beam.window.SlidingWindows(10, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can visualize (see [FAQ #3.How do I read the visualization](../../faq.md#q3)) the windowed JSON data by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this if you want to visualize the windowed JSON data:\n",
    "# ib.show(windowed_data, include_window_info=True, visualize_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see duplicate data for each element because each element has to appear in multiple\n",
    "windows with sliding windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the 10-second dollar run rate for each second, by summing the `meter_increment` JSON field for each window.\n",
    "\n",
    "First, extract the `meter_increment` field from the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meter_increments = windowed_data | beam.Map(lambda e: e.get('meter_increment'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can inspect the extracted data by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this if you want to inspect the extracted data:\n",
    "# ib.show(meter_increments, include_window_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sum all elements by window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rates = meter_increments | beam.CombineGlobally(sum).without_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the 10-second dollar run rate for each second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(run_rates, include_window_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can add a sink to `run_rates`, and run a [Google Cloud Dataflow](https://cloud.google.com/dataflow) job, and you'll have a continuous run rate PubSub feed!\n",
    "```\n",
    "run_rates | beam.io.WriteToPubSub(topic=<your-topic>)\n",
    "```\n",
    "\n",
    "Refer to the [user guide](https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development) on how to run a Dataflow job using a pipeline assembled from your notebook. You can also refer to [this walkthrough](Dataflow_Word_Count.ipynb) which is based on the [first word count example notebook](01-Word_Count.ipynb).\n",
    "\n",
    "Also, as mentioned in the beginning, there are many other fields in each element of the stream. As an exercise, try extracting other fields and applying your own computations.\n",
    "\n",
    "When you are done with this example, you might want to visit the [PubSub subscription page](https://console.cloud.google.com/cloudpubsub/subscription/list) to delete any subscription created by this example.\n",
    "\n",
    "If you have any feedback on this notebook, drop us a line at beam-notebooks-feedback@google.com."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
