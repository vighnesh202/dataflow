{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394e1cd2",
   "metadata": {},
   "source": [
    "##### Copyright 2021 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8b934-ed50-482e-bf36-fa65c766b990",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Beam SQL in notebooks\n",
    "\n",
    "This example demonstrates how to use SQL to write Apache Beam pipelines, use the `beam_sql` magic (since Beam v2.34.0) to quickly iterate the pipeline development in notebooks, and launch Dataflow jobs with Beam SQL from notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6783e4b-7f64-44fc-8734-fba455d3dcce",
   "metadata": {},
   "source": [
    "## An overview of Beam SQL\n",
    "\n",
    "[Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) allows a Beam user (currently only available in Beam Java and Python) to query bounded and unbounded PCollections with SQL statements. Your SQL query is translated to a PTransform, an encapsulated segment of a Beam pipeline. You can freely mix SQL PTransforms and other PTransforms in your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d1287-7737-4923-a7c9-d4b29008ebc1",
   "metadata": {},
   "source": [
    "Since SQL support in Beam Python SDK is implemented through xLang external transform, make sure you have below prerequisites:\n",
    "- Have `docker` installed;\n",
    "- Have jdk8 or jdk11 installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a6b21-cc5c-47f2-82d8-e4ec44d143de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The notebook environment should have docker and jdk 1.8 installed.\n",
    "!docker image list\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d5c83-d14e-4787-a850-de14f4075f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally sets the logging level to reduce distraction.\n",
    "import logging\n",
    "\n",
    "logging.root.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c15613-0722-4307-8fcf-81b490a3b68f",
   "metadata": {},
   "source": [
    "**Important**: If you're using Beam built from your local source code, additionally:\n",
    "\n",
    "- Have the Java expansion service shadowjar built. Go to the root directory of your local beam repo and then execute:\n",
    "  `./gradlew :sdks:java:extensions:sql:expansion-service:shadowJar`.\n",
    "- Based on your jdk version, pull the docker image `docker pull apache/beam_java11_sdk` or `docker pull apache/beam_java8_sdk`.\n",
    "- Then tag the image with your current Beam dev version.  You can check the dev version under `apache_beam.version.__version__`. For example, if you're using jdk11 and dev version is `x.x.x.dev`, execute `docker image tag apache/beam_java11_sdk:latest apache/beam_java11_sdk:x.x.x.dev`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba8473-a6be-4ffd-9707-281ea6614bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run Beam SQL in notebooks with `beam_sql` magic\n",
    "\n",
    "[Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) allows a Beam user to query PCollections with SQL statements. Currently, `InteractiveRunner` does not support `SqlTransform` due to [BEAM-10708](https://issues.apache.org/jira/browse/BEAM-10708). However, a user could use the `beam_sql` magic to run Beam SQL in the notebook and introspect the result. \n",
    "\n",
    "`beam_sql` is an IPython [custom magic](https://ipython.readthedocs.io/en/stable/config/custommagics.html). If you're not familiar with magics, here are some [built-in examples](https://ipython.readthedocs.io/en/stable/interactive/magics.html). It's a convenient way to validate your queries locally against known/test data sources when prototyping a Beam pipeline with SQL, before productionizing it on remote cluster/services.\n",
    "\n",
    "The notebook environment has preloaded the `beam_sql` magic. You can also explicitly load it via `%load_ext apache_beam.runners.interactive.sql.beam_sql_magics` if you set up your own notebook elsewhere.\n",
    "\n",
    "**Note**: `beam_sql` is for `apache_beam[interactive]>=2.34.0`. Make sure to connect your notebook to a kernel that meets the requirement.\n",
    "\n",
    "The `beam_sql` magic can be used as either a line magic or a cell magic. You can check its usage by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfb9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "%beam_sql -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf321d12-546d-47e3-be43-00dff46ee79d",
   "metadata": {},
   "source": [
    "The advantages of using `beam_sql` in notebooks are:\n",
    "- all scenarios use the same intuitive syntax and you don't have to differentiate them from each other any more\n",
    "    - no need to use the constant `PCOLLECTION` when querying a single PCollection\n",
    "    - no need to name multiple input PCollections, instead referring them by their variable names\n",
    "- no need to write SqlTransform and other Beam related boilerplates\n",
    "- can introspect the result immediately without running the pipeline explicitly or implicitily through a context manager\n",
    "- automatically handles coder registration for your PCollection schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3535b-75bb-44e5-8c24-f1a85b208e25",
   "metadata": {
    "tags": []
   },
   "source": [
    "### There are three scenarios for Beam SQL\n",
    "\n",
    "When writing Beam SQL, this notebook uses the `beam_sql` magic because of its advantages over the `SqlTransform`. *A comparison between using the `beam_sql` magic and `SqlTransform` to write Beam SQL can be found in the [appendix](#Beam-SQL-without-beam_sql-magic)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497237cb-0173-46e7-9332-4a87d0068409",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Use Beam SQL to create a PCollection from constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51474e-c974-4574-9b8e-7f5189832038",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o pcoll\n",
    "SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71499f-8892-42cb-acb4-40d336d4d252",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Use Beam SQL to query a PCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c650397-78ca-49b3-bf74-42e642c41725",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o id_pcoll\n",
    "SELECT id FROM pcoll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31af31-ed51-4a32-acf7-61b72a1d2ec8",
   "metadata": {},
   "source": [
    "#### 3. Use Beam SQL to query multiple PCollections (joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9af0bce-ecbe-4111-8463-00bd77f5f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o value_with_same_id\n",
    "SELECT * FROM pcoll JOIN id_pcoll USING (id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba5d714-4d12-4346-a821-66ffe19dae4d",
   "metadata": {},
   "source": [
    "## Example queries\n",
    "\n",
    "Now that you are familiar with the Beam SQL and the `beam_sql` magic, let's view a few more examples with advanced usage of the `beam_sql` magic such as mixing it with other Beam I/O connectors.\n",
    "\n",
    "### Query#1 - A simple static query\n",
    "\n",
    "You can run a simple SQL query (in Apache Calcite SQL [syntax](https://beam.apache.org/documentation/dsls/sql/calcite/query-syntax/)) to create a [schema-aware PCollection](https://beam.apache.org/documentation/programming-guide/#schemas) from constant values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6764a-674e-4454-ac47-c510a46bc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o query1_data\n",
    "SELECT CAST(5 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f298c-6151-44cf-9c6d-00927d725062",
   "metadata": {},
   "source": [
    "The `beam_sql` magic shows you the result of the SQL query.\n",
    "\n",
    "It also creates and outputs a PCollection named `query1_data` with `element_type` like `BeamSchema_...(id: int32, str: str)`.\n",
    "\n",
    "Note that you have **not** explicitly created a Beam pipeline. You get a PCollection because the `beam_sql` magic always **implicitly creates** a pipeline to execute your SQL query. To hold the elements with each field's type info, Beam automatically creates a schema as the `element_type` for the created PCollection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558537ff-f2f4-498b-b014-40c378f069f3",
   "metadata": {},
   "source": [
    "To introspect the data again with more knobs, you can use `show`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c26e43-9a7a-4a9b-b39b-9a6cfccae532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.runners.interactive import interactive_beam as ib\n",
    "ib.show(query1_data)\n",
    "# Uncomment below to set more args.\n",
    "# ib.show(query1_data, visualize_data=True, include_window_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc722d-da81-4fc2-b0c0-3b86319ce03a",
   "metadata": {},
   "source": [
    "To materialize the PCollection into a pandas [DataFrame](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe) object, you can use `collect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f105733-5ecd-4d37-89c7-173769283986",
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.collect(query1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c581e90-e636-4bca-80d8-069743b2c631",
   "metadata": {},
   "source": [
    "You can also additionally append some transforms such as writing to a text file and print the elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45724257-6ba0-4bd8-a0c8-95e19dd81546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "coder=beam.coders.registry.get_coder(query1_data.element_type)\n",
    "print(coder)\n",
    "query1_data | beam.io.textio.WriteToText('/tmp/query1_data', coder=coder)\n",
    "query1_data | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdaf489-0c9e-4eb3-817e-96c052774fd5",
   "metadata": {},
   "source": [
    "Execute the pipeline as a normal pipeline running on DirectRunner and inspect the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfbe6c-0126-4157-8d42-b2893c8f4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /tmp/query1_data*\n",
    "query1_data.pipeline.run().wait_until_finish()\n",
    "!ls /tmp/query1_data*\n",
    "!cat /tmp/query1_data*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d317a-aed2-451a-b5c6-626878bda0f5",
   "metadata": {},
   "source": [
    "The coder in use is a `RowCoder`. The element is encoded and written to the text file. When inspecting it directly, it may display **garbled strings**. The file will be revisited later in Query#4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8159b88-c79c-4d77-8966-388fc014f608",
   "metadata": {},
   "source": [
    "### [Optional] Omit the `-o` option.\n",
    "If the option is omitted, an output name is auto-generated based on the SQL query and PCollection (if any) it queries. Optionally, you can also use the `_[{execution_count}]` convention: `_` for last output and `_{execution_count}` for a specific cell execution output.\n",
    "\n",
    "However, explicitly naming the output is recommended for better notebook readability and to avoid unexpected errors.\n",
    "\n",
    "Below example outputs a PCollection named like `sql_output_...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ead5f-3de0-46a0-8e94-d1206390bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql\n",
    "SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda16c42-341b-481c-b704-6d9d3c76d048",
   "metadata": {},
   "source": [
    "Now that you are familiar with the `beam_sql` magic, you can build more queries against PCollections.\n",
    "\n",
    "Let's install the `names` package to randomly generate some names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ade39-dbbf-4cf6-967e-5f853570319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bb84a-be9e-4f3b-98fb-c7750e5b19d0",
   "metadata": {},
   "source": [
    "Import all modules needed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5c9b4-ddaa-4d2c-968c-a86c430cfdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "import typing\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "from apache_beam.runners.interactive import interactive_beam as ib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af4cd3-8ef3-4108-a76b-2e9006c0a935",
   "metadata": {},
   "source": [
    "Create a pipeline `p` with the `InteractiveRunner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f535b-0cfc-4e97-a092-0ff9142350f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8063b29-11ae-46e4-89db-436c98182b27",
   "metadata": {},
   "source": [
    "Then let's create a schema with `typing.NamedTuple`. Let's call it `Person` with a field `id` and a field `name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5e466-e5f3-4932-b4a1-b1cffbfa05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(typing.NamedTuple):\n",
    "    id: int\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f277a-e918-4e09-877d-534cdde45f71",
   "metadata": {},
   "source": [
    "With `beam_sql` magic, you can utilize all the Beam I/O connectors (streaming is currently not supported due to `DirectRunner` not supporting streaming pipeline with `SqlTransform`) as source of data, then build a SQL query against all the data and check the output. If needed, you can sink the output following the `WriteToText` example demonstrated above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b1e97-83bc-45e1-8d2c-c462bf5208ae",
   "metadata": {},
   "source": [
    "## Query#2 - Querying a single PCollection\n",
    "\n",
    "Let's build a PCollection with 10 random `Person` typed elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318c928-0192-4f6a-9f55-47593d683b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = (p \n",
    "           | beam.Create([Person(id=x, name=names.get_full_name()) for x in range(10)]))\n",
    "ib.show(persons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d883db-bc6a-441d-b380-36d683759d47",
   "metadata": {},
   "source": [
    "You can look for all elements with `id < 5` in `persons` with the below query and assign the output to `persons_id_lt_5`. Also, you can enable `-v` option to see more details about the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981207a6-e36e-4ee4-b330-5f46f0adbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o persons_id_lt_5 -v\n",
    "SELECT * FROM persons WHERE id <5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebac08-f141-4f60-a705-de98a7922fac",
   "metadata": {},
   "source": [
    "With `-v`, if it's the first time running this query, you might see a warning message about\n",
    "\n",
    "```\n",
    "Schema Person has not been registered to use a RowCoder. Automatically registering it by running: beam.coders.registry.register_coder(Person, beam.coders.RowCoder)\n",
    "```\n",
    "\n",
    "The `beam_sql` magic helps registering a `RowCoder` for each schema you define and use whenever it finds one. You can also explicitly run the same code to do so.\n",
    "\n",
    "Note the output element type is `Person(id: int, name: str)` instead of `BeamSchema_...` because you have selected all the fields from a single PCollection of the known type `Person(id: int, name: str)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8692e2-60b6-45bf-b91c-b4438d38673a",
   "metadata": {},
   "source": [
    "## Query#3 - Joining multiple PCollections\n",
    "\n",
    "You can build a `persons_2` PCollection with a different range of `id`s and `name`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80d164-93bb-4189-985d-8e0ccf4ee8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_2 = (p \n",
    "             | beam.Create([Person(id=x, name=names.get_full_name()) for x in range(5, 15)]))\n",
    "ib.show(persons_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429563f-45cf-4369-8213-5eda145cd1fa",
   "metadata": {},
   "source": [
    "Then query for all `name`s from `persons` and `persons_2` with the same `id`s and assign the output to `persons_with_common_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77317e-fb65-4682-aeca-c98e8f5099bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o persons_with_common_id -v\n",
    "SELECT * FROM persons JOIN persons_2 USING (id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd8d0a-126e-437c-b71b-6455c737c479",
   "metadata": {},
   "source": [
    "Note the output element type is now some `BeamSchema_...(id: int64, name: str, name0: str)`. Because you have selected columns from both PCollections, there is no known schema to hold the result. Beam automatically creates a schema and differentiates conflicted field `name` by suffixing `0` to one of them.\n",
    "\n",
    "And since `Person` is already previously registered with a `RowCoder`, there is no more warning about registering it anymore even with `-v`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a58e6-99e8-4168-97e5-43d7e0a68e8f",
   "metadata": {},
   "source": [
    "## Query#4 - Join multiple PCollections, including I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd84b50-35ef-465d-8e55-003332b642cf",
   "metadata": {},
   "source": [
    "Let's read the file written by Query#1 and use it to join `persons` and `persons_2` to find `name`s with the common `id` in all three of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe39750-92d6-4344-97c4-f00cb5982a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the exact same coder used when WriteToText and explicitly set the output types.\n",
    "query1_result_in_file = p | beam.io.ReadFromText(\n",
    "    '/tmp/query1_data*', coder=coder).with_output_types(\n",
    "    query1_data.element_type)\n",
    "\n",
    "# Check all the data sources.\n",
    "ib.show(query1_result_in_file)\n",
    "ib.show(persons)\n",
    "ib.show(persons_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe914a95-4aec-493d-a240-215ce696dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o entry_with_common_id\n",
    "\n",
    "SELECT query1_result_in_file.id, persons.name AS `name_1`, persons_2.name AS `name_2`\n",
    "FROM query1_result_in_file JOIN persons ON query1_result_in_file.id = persons.id\n",
    "JOIN persons_2 ON query1_result_in_file.id = persons_2.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af0a54-ae5b-4b5d-bbdc-e46f2da6464b",
   "metadata": {},
   "source": [
    "You can also chain another `beam_sql` magic to get just `name_1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d81190-8dcb-4b53-8cfc-d88512360309",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o name_found\n",
    "SELECT name_1 AS `name` FROM entry_with_common_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5649f-c046-4ab0-bb87-bd4c8e646a78",
   "metadata": {},
   "source": [
    "## Run Beam SQL on Dataflow via `beam_sql` magic\n",
    "\n",
    "Next you can execute the same SQL to pick only `name_1` column from the `entry_with_common_id` but run it on Dataflow by specifying `-r DataflowRunner`.\n",
    "\n",
    "A form will be generated below for you to fill in minimum pipeline options needed. Some of the fields might have been auto-populated based on the context of this notebook environment.\n",
    "\n",
    "There are 2 buttons:\n",
    "- `Run on Dataflow` submits a Dataflow job from this notebook.\n",
    "- `Show Options` shows you the current pipeline options configured for the job to be submitted.\n",
    "\n",
    "**Important**: If you're using Beam built from source code, you need to execute the cell after next cell to set `sdk_location` before clicking the `RUN ON DATAFLOW` button generated by this cell.\n",
    "\n",
    "**Tips**: In the form generated by the `beam_sql` magic, use `gs://your-GCS-bucket` as the `GCS Bucket` and put `names` in the `Additional Packages`. The output PCollection will be automatically saved to `gs://your-GCS-bucket/staging/on_dataflow` file on Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o on_dataflow -r DataflowRunner\n",
    "SELECT name_1 AS `name` FROM entry_with_common_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561c6cf-2e8d-492b-ba82-42be42f1743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and execute iff you're using Beam built from source code.\n",
    "# from apache_beam.options.pipeline_options import SetupOptions\n",
    "# options_on_dataflow.view_as(SetupOptions).sdk_location = '/dir/to/your/apache-beam-x.xx.x.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d57eef-86b0-42dc-830c-3513756cf8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace your-GCS-bucket with the real input and execute once the dataflow job is done.\n",
    "!gsutil cat 'gs://your-GCS-bucket/staging/abc-00000-of-00001'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ecf73c-e661-4b43-9e44-c5fe758c7cef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What's the difference between pipelines using Beam SQL and those not using it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a273d-b367-4e40-9804-536ee6f7fe33",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's build a pipeline to find out the data for the state with the most COVID positive cases on a specific day.\n",
    "### A pipeline without using Beam SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c487dfc-5958-4af9-8921-8a9496b1191e",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95aa0b0-b89c-4214-9140-43e7e80c64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# The covidtracking project has stopped collecting new data, current data ends on 2021-03-07\n",
    "json_current='https://covidtracking.com/api/v1/states/current.json'\n",
    "\n",
    "def get_json_data(url):\n",
    "  with requests.Session() as session:\n",
    "    data = json.loads(session.get(url).text)\n",
    "  return data\n",
    "\n",
    "current_data = get_json_data(json_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f0d18-094d-4fdc-8bd9-4f62d31ca117",
   "metadata": {},
   "source": [
    "#### Create a PCollection from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4100ef83-4317-49ca-bc92-151aae1a0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(runner=InteractiveRunner())\n",
    "raw_data = p | 'Create PCollection from json' >> beam.Create(current_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b540458-2174-4f21-bd59-f4c7c62215cd",
   "metadata": {},
   "source": [
    "#### Create a orderable wrapper for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268bc804-9865-4680-9029-df8611368aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering\n",
    "\n",
    "@total_ordering\n",
    "class UsCovidDataOrderByPositive:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.data['positive'] > other.data['positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333e15b-924b-410a-89d7-2b8e8ec211b3",
   "metadata": {},
   "source": [
    "#### Pick 4 columns from the data\n",
    "\n",
    "Below code uses a plain dictionary to hold the data.\n",
    "\n",
    "You don't have to define a schema nor explicitily be aware of the type of each field. \n",
    "\n",
    "But this can be dangerous when you write a more complicated pipeline where your assumptions about the data is incorrect.\n",
    "\n",
    "With `visualize_data=True` in `show()`, you can spot the element with the maximum positive in the visualization by setting `Color By positive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c2d32-1b61-4daf-a07c-0d5d20dc1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data | 'Parse' >> beam.Map(\n",
    "    lambda e: {\n",
    "        'date': e['date'], \n",
    "        'state': e['state'], \n",
    "        'positive': e['positive'], \n",
    "        'negative': e['negative']})\n",
    "ib.show(data, visualize_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1d18e-2d41-4297-bb42-f1745b84a329",
   "metadata": {},
   "source": [
    "Find the element with the most positive.\n",
    "\n",
    "This is rather verbose, you have to wrap the data into an orderable wrapper, find the maximum entry by the ordering and then unwrap the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df7e83-deb5-4791-8295-827121029693",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_with_max_positive = (\n",
    "    data | 'Data OrderByPositive' >> beam.Map(lambda e: UsCovidDataOrderByPositive(e))\n",
    "         | 'Find Maximum Positive' >> beam.CombineGlobally(max)\n",
    "         | 'Convert Back to Data' >> beam.Map(lambda orderable_data: orderable_data.data))\n",
    "ib.show(entry_with_max_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08abaf4e-0c10-4c71-a998-0c476b3ccf23",
   "metadata": {},
   "source": [
    "### A pipeline using Beam SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987611d-2238-41c8-b238-673ff0cd6847",
   "metadata": {},
   "source": [
    "Let's build the same pipeline but with Beam SQL.\n",
    "\n",
    "There is something wrong with the below schema. It works with normal Beam usage in Python but it doesn't work with Beam SQL. Can you spot the mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c039470-7a8a-4298-9da7-be395c2d3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class UsCovidData(NamedTuple):\n",
    "    date: str\n",
    "    state: str\n",
    "    positive: int\n",
    "    negative: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cad52b2-35e9-4288-a8b4-85a57dcbca39",
   "metadata": {},
   "source": [
    "The answer:\n",
    "\n",
    "- `date` is a keyword in (Calcite)SQL, use a different field name such as `partition_date`;\n",
    "- `date` from the data is an integer type, not str. Make sure you convert the data using `str()` or use `date: int`.\n",
    "- `negative` has missing values and the default is None. So instead of `negative: int`, it should be `negative: Optional[int]`. Or you can convert None into 0 when using the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56db725-a3ff-4e72-8779-7be1ef3e5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "# Adjusted schema based on the data\n",
    "class UsCovidData(NamedTuple):\n",
    "    partition_date: str  # Remember to str(e['date']).\n",
    "    state: str\n",
    "    positive: int\n",
    "    negative: Optional[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6aa8cb-33e6-4d85-8760-f8be6531bee9",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425cd72-1e3e-4a35-8b6d-cd27284e811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sql = beam.Pipeline(runner=InteractiveRunner())\n",
    "covid_data = (p_sql \n",
    "        | 'Create PCollection from json' >> beam.Create(current_data)\n",
    "        | 'Parse' >> beam.Map(\n",
    "            lambda e: UsCovidData(\n",
    "                partition_date=str(e['date']),\n",
    "                state=e['state'],\n",
    "                positive=e['positive'],\n",
    "                negative=e['negative'])).with_output_types(UsCovidData))\n",
    "ib.show(covid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cc5158-6e35-4390-8522-fba7285e438d",
   "metadata": {},
   "source": [
    "#### Find the maximum positive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a896dff-b2d8-4a79-9de5-35b3be3c769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o max_positive\n",
    "SELECT partition_date, MAX(positive) AS `positive`\n",
    "FROM covid_data\n",
    "GROUP BY partition_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4202b7-99a3-4320-83c3-2ad897a4ba6e",
   "metadata": {},
   "source": [
    "#### Join the maximum positive value with the original data to get the rest fields.\n",
    "\n",
    "Below code also handles the `negative is None` case to use a default value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd1e01-0521-44f8-8e7a-6dc65054008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%beam_sql -o entry_with_max_positive\n",
    "SELECT covid_data.partition_date, covid_data.state, covid_data.positive, {fn IFNULL(covid_data.negative, 0)} AS `negative`\n",
    "FROM covid_data JOIN max_positive\n",
    "USING (partition_date, positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e74af9-eaec-4715-9e18-662d899419cb",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Beam SQL without `beam_sql` magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0514c021-c175-4dae-82e1-33e496ede19a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Use Beam SQL to create a PCollection from constant values\n",
    "\n",
    "Note the `SqlTransform` is applied to a pipeline not a PCollection, unlike the below 2 scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a421ced-1769-4374-a543-87d7a60fceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    pcoll = p | 'Create pcoll' >> SqlTransform(\"\"\"\n",
    "        SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`\"\"\")\n",
    "    _ = pcoll | beam.io.WriteToText('/tmp/pcoll')\n",
    "\n",
    "!cat /tmp/pcoll*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12113310-45cf-44b5-a846-56d24398e32f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Use Beam SQL to query a PCollection\n",
    "\n",
    "You have to use the `PCOLLECTION` constant in the query.\n",
    "\n",
    "Below query selects `id` field from the above `pcoll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dce749-0b67-403a-80c4-2cbe86805e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    pcoll = p | 'Create pcoll' >> SqlTransform(\"\"\"\n",
    "        SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`\"\"\")\n",
    "    id_pcoll = pcoll | 'Select id from pcoll' >> SqlTransform(\"\"\"SELECT id FROM PCOLLECTION\"\"\")\n",
    "    _ = id_pcoll | beam.io.WriteToText('/tmp/id')\n",
    "\n",
    "!cat /tmp/id*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490be2c3-eead-45f2-a482-75620a9902ee",
   "metadata": {},
   "source": [
    "#### 3. Use Beam SQL to query multiple PCollections (joined)\n",
    "You can tag PCollections with names and refer them in the query using the tagged names.\n",
    "\n",
    "Below query joins previous 2 PCollections by `id`, should return the original element from `pcoll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e6652-cf42-4f5b-ac48-6cfb58468733",
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    pcoll = p | 'Create pcoll' >> SqlTransform(\"\"\"\n",
    "        SELECT CAST(1 AS INT) AS `id`, CAST('foo' AS VARCHAR) AS `str`, CAST(3.14 AS DOUBLE) AS `flt`\"\"\")\n",
    "    id_pcoll = pcoll | 'Select id from pcoll' >> SqlTransform(\"\"\"SELECT id FROM PCOLLECTION\"\"\")\n",
    "    value_with_same_id = {'input_1': pcoll, 'input_2': id_pcoll} | 'Join pcolls' >> SqlTransform(\"\"\"\n",
    "        SELECT * FROM input_1 JOIN input_2 USING (id)\"\"\")\n",
    "    _ = value_with_same_id | beam.io.WriteToText('/tmp/same_id')\n",
    "\n",
    "!cat /tmp/same_id*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
