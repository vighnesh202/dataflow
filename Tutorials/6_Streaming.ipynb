{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "These notebooks shows two examples of streaming pipelines, one using the `InteractiveRunner` and one using the `DataflowRunner`.\n",
    "\n",
    "Before getting into the code, let's prepare the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub\n",
    "from apache_beam.io.gcp.bigquery import BigQueryDisposition, WriteToBigQuery\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "import google.auth\n",
    "\n",
    "from utils.utils import publish_to_topic\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing from batch to streaming in Apache Beam is quite easy, you only need to specify this option or add the flag `--streaming` if launching it from a terminal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The project will be used for creating a subscription to the Pub/Sub topic and for the Dataflow pipeline\n",
    "project = google.auth.default()[1]\n",
    "\n",
    "options = pipeline_options.PipelineOptions(\n",
    "    streaming=True,\n",
    "    project=project\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the pipeline uses an unbounded source (Pub/Sub), the next cell sets the maximum duration you want to record the source for replayability without exhausting resources because the data is indefinite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.options.recording_duration = '1m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are ready to launch a pipeline using `InteractiveRunner`. Whenever you want to stop it, you'd need to press the *Stop* on the top left of the notebook or press *i, i*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner(), options=options)\n",
    "\n",
    "topic = \"projects/pubsub-public-data/topics/taxirides-realtime\"\n",
    "\n",
    "pubsub = (p | \"Read Topic\" >> ReadFromPubSub(topic=topic)\n",
    "            | beam.Map(json.loads))\n",
    "\n",
    "ib.show(pubsub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________\n",
    "Until now we have been run been running the code using runner `InteractiveRunner`, meaning the pipelines are executed in this very notebook. The next pipeline will be executed in Dataflow using `DataflowRunner`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the pipeline is going to run outside the notebook, let's change the level of logging to get more visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also going to need some resources from Google Cloud. The next cell creates a Cloud Storage bucket, a BigQuery Dataset and a Pub/Sub topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb  gs://beam-basics-{project}\n",
    "\n",
    "!bq mk --location US --dataset beam_basics \n",
    "    \n",
    "!gcloud pubsub topics create beambasics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to specify some more options that Dataflow requires, as `temp_location` or `region`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_pipeline(project, region=\"us-central1\"):\n",
    "    \n",
    "    topic = \"projects/{}/topics/beambasics\".format(project)\n",
    "    table = \"{}:beam_basics.from_pubsub\".format(project)\n",
    "    schema = \"name:string,score:integer,timestamp:timestamp\"\n",
    "    bucket = \"gs://beam-basics-{}\".format(project)\n",
    "    \n",
    "    options = PipelineOptions(\n",
    "        streaming=True,\n",
    "        project=project,\n",
    "        region=region,\n",
    "        staging_location=\"%s/staging\" % bucket,\n",
    "        temp_location=\"%s/temp\" % bucket\n",
    "    )\n",
    "\n",
    "    p = beam.Pipeline(DataflowRunner(), options=options)\n",
    "\n",
    "    pubsub = (p | \"Read Topic\" >> ReadFromPubSub(topic=topic)\n",
    "                | \"To Dict\" >> beam.Map(json.loads)) # Example message: {\"name\": \"carlos\", 'score': 10, 'timestamp': \"2020-03-14 17:29:00.00000\"}\n",
    "\n",
    "    pubsub | \"Write To BigQuery\" >> WriteToBigQuery(table=table, schema=schema,\n",
    "                                  create_disposition=BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                  write_disposition=BigQueryDisposition.WRITE_APPEND)\n",
    "\n",
    "    return p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pipeline = streaming_pipeline(project)\n",
    "    print(\"\\n PIPELINE RUNNING \\n\")\n",
    "except (KeyboardInterrupt, SystemExit):\n",
    "    raise\n",
    "except:\n",
    "    print(\"\\n PIPELINE FAILED\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's a running streaming pipeline. Since the pipeline reads from Pub/Sub, you need to publish some messages. The publisher is in the `utils.py` file, which is already imported.\n",
    "\n",
    "As an example, the elements published have this form:\n",
    "\n",
    "`{'name': 'carlos' (string), 'score': 22(int), 'timestamp':'2020-03-14 17:29:00.00000' (string)}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_messages = 1000\n",
    "print(\"\\nLet's wait a bit so the workers can start up.\\n\")\n",
    "time.sleep(30)\n",
    "print(\"Ok, let's start publishing.\\n\")\n",
    "try:\n",
    "    publish_to_topic(num_messages, \"beambasics\", project, notebook_number=6, time_division=10)\n",
    "    print(\"\\n PUBLISHING DONE\\n\")\n",
    "except (KeyboardInterrupt, SystemExit):\n",
    "    raise\n",
    "except:\n",
    "    print(\"\\n PUBLISHING FAILED\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go to your Google Cloud project and check the Dataflow job status. You can also check the BigQuery table (it may take some time until data is available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\"https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s\" %\n",
    " (pipeline._job.location, pipeline._job.id,\n",
    "  pipeline._job.projectId))\n",
    "display(HTML(\"Click <a href=\\\"%s\\\" target=\\\"_new\\\">here</a> for the details of your Dataflow job.\" % url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq query --location US --use_legacy_sql=false 'SELECT * FROM `beam_basics.from_pubsub` LIMIT 10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to publish more messages before stopping the Dataflow job, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_messages = int(input(\"How many messages you want to publish? \") or 0)\n",
    "publish_to_topic(extra_messages, \"beambasics\", project, notebook_number=6, time_division=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to cancel the running pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.cancel()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
