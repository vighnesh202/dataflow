{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time has come to test everything you learned during these notebooks (which we hope is a lot).\n",
    "\n",
    "The next exercises are the `hello world` of ETL pipelines: a **WordCount**. First as usual, but then you'll add a modification. You will also have a **streaming** exercise.\n",
    "\n",
    "As always, there are many possible solutions to this, so it's fine if your solution doesn't match the ones posted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "from utils.utils import *\n",
    "from utils.solutions import solutions\n",
    "import google.auth\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam import FlatMap, Map, ParDo, Flatten, Filter\n",
    "from apache_beam import Values, CombineGlobally, CombinePerKey\n",
    "from apache_beam import pvalue, window, WindowInto\n",
    "from apache_beam.options import pipeline_options\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.transforms.combiners import Top, Mean, Count\n",
    "from apache_beam.io.textio import ReadFromText, WriteToText\n",
    "from apache_beam.io.gcp.pubsub import ReadFromPubSub\n",
    "from apache_beam.io.gcp.bigquery import BigQueryDisposition, WriteToBigQuery\n",
    "\n",
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "\n",
    "from apache_beam.testing.util import assert_that, is_empty, equal_to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard WordCount\n",
    "\n",
    "The pipeline is going to read file `kinglear.txt` from a public Cloud Storage bucket and output the number of times each word appears.\n",
    "\n",
    "**NOTE**: The pipeline is counting the words case sensitive, i.e., \"Friend\" and \"friend\" are ***not*** counted together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "    \n",
    "path = \"gs://dataflow-samples/shakespeare/kinglear.txt\"\n",
    "\n",
    "def split_words(text):\n",
    "    words = re.findall(r'[\\w\\']+', text.strip(), re.UNICODE)\n",
    "    return #TODO \n",
    "\n",
    "#TODO finish pipeline\n",
    "count = (p \n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "wc =  (count | \"Filter\" >> Filter(lambda x: x[0] in words_test))\n",
    "\n",
    "assert_that(wc, equal_to(solutions[7][\"wordcount\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `InteractiveRunner` you can even visualize the output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(count, visualize_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process elements**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    " \n",
    "The notebook about [I/O operations](5_IOOperations.ipynb) showed that when using `ReadFromText`, it read by lines. So you need to from every line (one element) output the words (more than one element). This is a `FlatMap`.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "count = (p | \"ReadTxt\" >> ReadFromText(path)\n",
    "           | \"Split Words\" >> FlatMap(split_words)\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split words return**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "\n",
    "Variable `words` is a list of the words in each text line, but this is not quite the output you need, you need key-value pairs as an output ( * ) so you can count them properly; the key is the word, and the value can be `1` (**).\n",
    "    \n",
    "(*) You don't actually need key-value pairs in this case, at the end there is another way of doing the same without key-value pairs.\n",
    "    \n",
    "(**) Depending on how we process the key-value pairs, you need to set `1` as value or set whatever as value. Another solution is shared with each example. The solution below accepts whichever value.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "    \n",
    "```  \n",
    "    def split_words(text):\n",
    "        words = re.findall(r'[\\w\\']+', text.strip(), re.UNICODE)\n",
    "        return [(x, 1) for x in words]    \n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full code**\n",
    "\n",
    "<details><summary>Solution 1</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "p = beam.Pipeline(InteractiveRunner())\n",
    "    \n",
    "path = \"gs://dataflow-samples/shakespeare/kinglear.txt\"\n",
    "\n",
    "def split_words(text):\n",
    "    words = re.findall(r'[\\w\\']+', text.strip(), re.UNICODE)\n",
    "    return [(x, 1) for x in words]\n",
    "\n",
    "count = (p | \"ReadTxt\" >> ReadFromText(path)\n",
    "           | \"Split Words\" >> FlatMap(split_words)\n",
    "           | \"Count\" >> Count.PerKey())\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "wc =  (count | \"Filter\" >> Filter(lambda x: x[0] in words_test))\n",
    "\n",
    "assert_that(wc, equal_to(solutions[7]['wordcount']))\n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "<details><summary>Solution 2</summary>\n",
    "<p>\n",
    "\n",
    "This solution doesn't require to output key-value pairs in the FlatMap. The reason why is because it uses `Count.PerElement()`, which count the number of occurrences of each distinct element in the PCollection.\n",
    "\n",
    "```\n",
    "p = beam.Pipeline(InteractiveRunner())\n",
    "    \n",
    "path = \"gs://dataflow-samples/shakespeare/kinglear.txt\"\n",
    "\n",
    "def split_words(text):\n",
    "    words = re.findall(r'[\\w\\']+', text.strip(), re.UNICODE)\n",
    "    return words\n",
    "\n",
    "count = (p | \"ReadTxt\" >> ReadFromText(path)\n",
    "   | \"Split Words\" >> FlatMap(split_words)\n",
    "   | \"Count\" >> Count.PerElement())\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "wc =  (count | \"Filter\" >> Filter(lambda x: x[0] in words_test))\n",
    "\n",
    "assert_that(wc, equal_to(solutions[7]['wordcount']))\n",
    "```\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution 3</summary>\n",
    "<p>\n",
    "    \n",
    "This solution uses the `CombinePerKey` rather than `Count`. It's a lower level solution and it can be easily modified to do other operations.\n",
    "\n",
    "```\n",
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "path = \"gs://dataflow-samples/shakespeare/kinglear.txt\"\n",
    "\n",
    "def split_words(text):\n",
    "    words = re.findall(r'[\\w\\']+', text.strip(), re.UNICODE)\n",
    "    return [(x, 1) for x in words]\n",
    "\n",
    "count = (p | \"ReadTxt\" >> ReadFromText(path)\n",
    "           | \"Split Words\" >> FlatMap(split_words)\n",
    "           | \"Count\" >> CombinePerKey(sum))\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "wc =  (count | \"Filter\" >> Filter(lambda x: x[0] in words_test))\n",
    "\n",
    "assert_that(wc, equal_to(solutions[7]['wordcount']))\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified wordcount\n",
    "\n",
    "Now let's spice things up. The pipeline counts words from two different sources: `kinglear.txt` and `hamlet.txt` (both in the same public Cloud Storage bucket) but it doesn't count all words, stop words will be discarded (i.e., \"and\", \"for\", \"to\",...). The stop words list is stored in a file (locally, in the `input` folder), and you may add or take some words out of that file `stopwords.txt`, the pipeline has to consider this. This time you will store the output locally in a file.\n",
    "\n",
    "Before starting coding, we recommend you to go and check the stop words file, to be able to process it.\n",
    "\n",
    "The posted solution uses as a base the previous solution using `Count.PerElement()`.\n",
    "\n",
    "**NOTE**: even though the pipeline is counting the words case sensitive, it is not doing it for stopwords, i.e., both \"To\" and \"to\" should be removed. You can use `<string>.lower()` to turn words into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "          \n",
    "path_1 = \"gs://dataflow-samples/shakespeare/kinglear.txt\"\n",
    "path_2 = \"gs://dataflow-samples/shakespeare/hamlet.txt\"\n",
    "stopwords_path = \"input/stopwords.txt\"\n",
    "\n",
    "output_path = \"Output/modified_wordcount\"\n",
    "\n",
    "# TODO: Finish pipeline\n",
    "\n",
    "count_no_stopwords = (p | )\n",
    "\n",
    "# Do the file writing here                      \n",
    "write = count_no_stopwords\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "filtered = count_no_stopwords | \"Filter\" >> Filter(lambda x: x[0] in words_no_sw_test)\n",
    "stop_words = count_no_stopwords | \"Get StopWords\" >> Filter(lambda x: x[0] in sw_test)\n",
    "\n",
    "assert_that(filtered, equal_to(solutions[7][\"modified_wordcount\"]), label=\"words\")\n",
    "assert_that(stop_words, is_empty(), label=\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process stop words**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "\n",
    "Each word in the stop word file is separated using `\", \"`, so you can split by that. Since from one element you need more elements, you can use `FlapMap`\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "stopwords_p = (p | \"Read Stop Words\" >> ReadFromText(stopwords_path)\n",
    "                 | FlatMap(lambda x: x.split(\", \"))) \n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split words with condition**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "\n",
    "This is the same situation as before in which you need to use `FlatMap` (`ParDo` also works, of course). Since you have a condition, you need to use a dynamic parameter, which translates as `Side Inputs`. What you need is the list of stop words, so use `AsList` when using the pipeline as `Side Input`.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "      \n",
    "    def split_words(text, stopwords):\n",
    "        words = re.findall(r'[\\w\\']+', text.strip(), re.UNICODE)\n",
    "        return [x for x in words if x.lower() not in stopwords]\n",
    "\n",
    "     {..}  | \"Split Words\" >> FlatMap(split_words, stopwords=beam.pvalue.AsList(stopwords_p))\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full code**\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "p = beam.Pipeline(InteractiveRunner())\n",
    "    \n",
    "path_1 = \"gs://dataflow-samples/shakespeare/kinglear.txt\"\n",
    "path_2 = \"gs://dataflow-samples/shakespeare/hamlet.txt\"\n",
    "stopwords_path = \"input/stopwords.txt\"\n",
    "\n",
    "output_path = \"Output/modified_wordcount\"\n",
    "\n",
    "def split_words(text, stopwords):\n",
    "    words = re.findall(r'[\\w\\']+', text.strip(), re.UNICODE)\n",
    "    return [x for x in words if x.lower() not in stopwords]\n",
    "\n",
    "stopwords_p = (p | \"Read Stop Words\" >> ReadFromText(stopwords_path)\n",
    "                 | FlatMap(lambda x: x.split(\", \"))) \n",
    "\n",
    "text_1 = p | \"Read Text 1\" >> ReadFromText(path_1)\n",
    "text_2 = p | \"Read Text 2\" >> ReadFromText(path_2)\n",
    "\n",
    "count_no_stopwords = ((text_1, text_2)  | Flatten()  \n",
    "                                        | \"Split Words\" >> FlatMap(\n",
    "                                            split_words, \n",
    "                                            stopwords=beam.pvalue.AsList(stopwords_p))\n",
    "                                        | \"Count\" >> Count.PerElement())\n",
    "\n",
    "write = count_no_stopwords | \"Write\" >> WriteToText(file_path_prefix=output_path, file_name_suffix=\".txt\")\n",
    "\n",
    "ib.show(count_no_stopwords, visualize_data=True)\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "filtered = count_no_stopwords | \"Filter\" >> Filter(lambda x: x[0] in words_no_sw_test)\n",
    "stop_words = count_no_stopwords | \"Get StopWords\" >> Filter(lambda x: x[0] in sw_test)\n",
    "\n",
    "assert_that(filtered, equal_to(solutions[7][\"modified_wordcount\"]), label=\"words\")\n",
    "assert_that(stop_words, is_empty(), label=\"stopwords\")\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Exercise\n",
    "\n",
    "The streaming pipeline is going to read from other topic we will create (`beambasics-exercise`). The structure of the messages is `{name (string), spent (integer)}` (messages are parsed in the pipeline).\n",
    "\n",
    "It needs to calculate the total amount each buyer (`name`) spends every minute, and write it to BigQuery.\n",
    "\n",
    "**Important note**: PubSubIO already adds the timestamp to the element (sent time), so you **don't** need to add the timestamp manually with `window.TimestampedValue`.\n",
    "\n",
    "Let's create the Pub/Sub topic first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create beambasics-exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_pipeline(project_param, region=\"us-central1\"):\n",
    "    topic = \"projects/{}/topics/beambasics-exercise\".format(project_param)\n",
    "    bucket = \"gs://beam-basics-{}\".format(project_param)\n",
    "    table = \"{}:beam_basics.exercise\".format(project_param)\n",
    "    schema = \"name:string,total_spent:integer\"\n",
    "\n",
    "    options = PipelineOptions(\n",
    "        streaming=True,\n",
    "        project=project,\n",
    "        region=region,\n",
    "        staging_location=\"%s/staging\" % bucket,\n",
    "        temp_location=\"%s/temp\" % bucket\n",
    "    )\n",
    "        \n",
    "    p = beam.Pipeline(DataflowRunner(), options=options)\n",
    "        \n",
    "    #TODO: Finish pipeline\n",
    "    pubsub = (p | \"Read Topic\" >> ReadFromPubSub(topic=topic)\n",
    "                | \"To dict\" >> Map(json.loads) # Example message: {\"name\": \"Guillem\", \"spent\": 10}\n",
    "                | \"To KV\" >> Map(lambda x: x[\"name\"], int(x[\"spent\"])))\n",
    "    \n",
    "    \n",
    "    return p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if the pipeline works, run the following cell. (Hints are below)\n",
    "\n",
    "The publisher is already imported from file `utils.py`. It should take about five minutes to finish all the messages (the publisher is throttled), so take this time to check if the outputs of the pipeline are right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_messages = 600\n",
    "project = google.auth.default()[1]\n",
    "try:\n",
    "    pipeline = streaming_pipeline(project)\n",
    "    print(\"\\n PIPELINE RUNNING \\n\")\n",
    "    url = (\"https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s\" %\n",
    "     (pipeline._job.location, pipeline._job.id,\n",
    "      pipeline._job.projectId))\n",
    "    display(HTML('Click <a href=\"%s\" target=\"_new\">here</a> for the details of your Dataflow job!' % url))\n",
    "    print(\"\\nLet's wait a bit so the workers can start up \\n\")\n",
    "    time.sleep(30)\n",
    "    print(\"Ok, let's start the publishing!\\n\")\n",
    "    try:\n",
    "        publish_to_topic(num_messages, \"beambasics-exercise\", project, notebook_number=7)\n",
    "        print(\"\\n PUBLISHING DONE\\n\")\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        raise\n",
    "    except:\n",
    "        print(\"\\n PUBLISHING FAILED\")\n",
    "        traceback.print_exc()\n",
    "except (KeyboardInterrupt, SystemExit):\n",
    "    raise\n",
    "except:\n",
    "    print(\"\\n PIPELINE FAILED\")\n",
    "    traceback.print_exc()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate total**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "\n",
    "Since you want to get the total of money spent, you need to sum the values, this is done with a `CombinePerKey`. But, since you are using streaming, you need to add Windows for Aggregations.  \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "     pubsub | \"FixedWindow\" >> WindowInto(window.FixedWindows(60))\n",
    "            | \"Sum\" >> CombinePerKey(sum)\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write to BigQuery**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "\n",
    "You need to do two things in order to write to BigQuery: prepare the elements and then actually write to BigQuery. When using Python, `WriteToBigQuery` takes either dictionaries or `TableRows` ([doc](https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.bigquery.html?highlight=tablerow#apache_beam.io.gcp.bigquery.TableRowJsonCoder)), this solution uses dictionaries.\n",
    "    \n",
    "Check the schema to be sure that you don't get errors. \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "      \n",
    "    def prepare_for_bq(element):\n",
    "        dictionary = {\n",
    "            \"name\": element[0],\n",
    "            \"total_spent\": element[1],\n",
    "          }\n",
    "        return dictionary\n",
    "        \n",
    "    {..}\n",
    "    \n",
    "                | \"Prepare for BigQuery\" >> Map(prepare_for_bq)\n",
    "                | \"Write To BigQuery\" >> WriteToBigQuery(table=table, schema=schema,\n",
    "                                  create_disposition=BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                  write_disposition=BigQueryDisposition.WRITE_APPEND))\n",
    "    \n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full code**\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "<p>\n",
    "    \n",
    "```\n",
    "def streaming_pipeline(project_param, region=\"us-central1\"):\n",
    "    \n",
    "    topic = f\"projects/{project_param}/topics/beambasics-exercise\"\n",
    "    bucket = f\"gs://beam-basics-{project_param}\"\n",
    "    table = f\"{project_param}:beam_basics.exercise\"\n",
    "    schema = \"name:string,total_spent:integer\"\n",
    "    \n",
    "    options = PipelineOptions(\n",
    "        streaming=True,\n",
    "        project=project,\n",
    "        region=region,\n",
    "        staging_location=\"%s/staging\" % bucket,\n",
    "        temp_location=\"%s/temp\" % bucket\n",
    "    )\n",
    "        \n",
    "    p = beam.Pipeline(DataflowRunner(), options=options)\n",
    "        \n",
    "    def prepare_for_bq(element):\n",
    "        dictionary = {\n",
    "            \"name\": element[0],\n",
    "            \"total_spent\": element[1],\n",
    "          }\n",
    "        return dictionary\n",
    "        \n",
    "    pubsub = (p | \"Read Topic\" >> ReadFromPubSub(topic=topic)\n",
    "                | \"To dict\" >> Map(json.loads)\n",
    "                | \"To KV\" >> Map(lambda x: (x[\"name\"], int(x[\"spent\"]))) # Example message: {\"name\": \"Guillem\", \"spent\": 10}\n",
    "                | \"FixedWindow\" >> WindowInto(window.FixedWindows(60))\n",
    "                | \"Sum\" >> CombinePerKey(sum)\n",
    "                | \"Prepare for BigQuery\" >> Map(prepare_for_bq)\n",
    "                | \"Write To BigQuery\" >> WriteToBigQuery(table=table, schema=schema,\n",
    "                                      create_disposition=BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                      write_disposition=BigQueryDisposition.WRITE_APPEND))\n",
    " \n",
    "    return p.run()\n",
    "\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember to shut down the pipeline when you are done with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.cancel()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
