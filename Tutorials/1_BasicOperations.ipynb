{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Beam basics\n",
    "\n",
    "In this first notebook you can explore basic operations within Apache Beam. After going through them, work through the exercises to test your knowledge.\n",
    "\n",
    "The following concepts are used throughout the notebooks:\n",
    "- **Element**: minimal unit of data.\n",
    "- **`PCollection`**: represents a distribute data set; it can be *bounded* or *unbounded*. Made of element(s).\n",
    "    - *Bounded* `PCollection` is data that has a fixed size. For example, text files, BigQuery tables, Avro files, and so on.\n",
    "    - *Unbounded* `PCollections` are potentially of infinite size, coming from a data stream. Examples of this are Pub/Sub topic/subscription and Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running into code, examine the basic structure for creating a pipeline:\n",
    "\n",
    "- At the beginning, to define your pipeline, use `p = beam.Pipeline()`.\n",
    "- The pipe `|` separates steps within the pipeline. Every time you want to add a new step, you need a new pipe.\n",
    "- At the right of the pipe, add the step you want to execute, ` | <STEP> `. You can optionally name the step using `>>` between the step and the pipe ` | \"NAME\" >> <STEP>`. Two steps cannot have the same name.\n",
    "- At the left of the pipe, there has to be a reference to a pipeline `p | <STEP>`, `p | <STEP1> | <STEP2>...` or `squares | <STEP>` (where squares is a pipeline variable ).\n",
    "____________________\n",
    "\n",
    "First, let's import the operations you need for this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam import Create, Map, ParDo, Flatten, Partition\n",
    "from apache_beam import pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the amount of logs for your pipeline, specify that only warnings are logged. For a greater level of details, change `WARNING` to `INFO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tutorials use the `InteractiveRunner` in most pipelines, so that you can see their graphs and output. Apache Beam can use other runners such as `DirectRunner`, `DataflowRunner`, or `FlinkRunner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package `sympy` is not part of Apache Beam, but it will be used in some examples. The following cell installs `sympy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Create`** is used to create elements.\n",
    "\n",
    "**`Map`** does an operation at the element level. Applies a simple one-to-one mapping function over each element in the collection.\n",
    "\n",
    "During most of the following examples we are going to use `Create` as the source in these examples, since it's more intuitive than using CSVs or other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pipeline returns the squares of the N first non-negative integers. For this first pipeline, we are going to use the default runner (`DirectRunner`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline()\n",
    "N = 7\n",
    "squares = (p | \"Create Elements\" >> Create(range(N))\n",
    "             | \"Squares\" >> Map(lambda x: x**2)\n",
    "             | Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the pipeline hasn't been executed; you need to use `p.run()` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the `with` statement as a context manager, you don't need to specify `p.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    squares = (p | \"Create Elements\" >> Create(range(N))\n",
    "                 | \"Squares\" >> Map(lambda x: x**2)\n",
    "                 | Map(print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the runner to `InteractiveRunner` is quite easy. `InteractiveRunner` has built-in functionalities like showing the pipeline graph or seeing the output without the need of `print`. You also don't need to use `p.run()`, since it's contained in `ib.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "squares = (p | \"Create Elements\" >> Create(range(N))\n",
    "             | \"Squares\" >> Map(lambda x: x**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the Pipeline Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, the examples use the `ib.show` from the`InteractiveRunner`.\n",
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ParDo`** is a more general operation than `Map` and the lowest level element-wise operation. It applies a given function to an element and outputs zero or more elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "N = 8\n",
    "\n",
    "def divisors(element):\n",
    "    divisor_list = sympy.divisors(element)\n",
    "    return [(element, x) for x in divisor_list]  # has to be an iterable\n",
    "\n",
    "divisors = (p | \"Create\" >> Create(range(N))\n",
    "              | \"ParDo Divisors\" >> ParDo(divisors))\n",
    "\n",
    "ib.show_graph(p)\n",
    "ib.show(divisors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that from N elements, the output is more than N elements (one-to-many). This operation could not be done with Map, since it's one-to-one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branching Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Flatten`** combines two or more `PCollections` into one. It takes elements for all input `PCollections` and outputs them as one `PCollection`. It's the equivalent of `UNION` in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "elements_1 = [\n",
    "    {\"country\": \"China\", \"population\": 1389},\n",
    "    {\"country\": \"India\", \"population\": 1311},\n",
    "    {\"country\": \"USA\", \"population\": 331},\n",
    "    {\"country\": \"Ireland\", \"population\": 5}\n",
    "]\n",
    "\n",
    "elements_2 = [\n",
    "    {\"country\": \"Indonesia\", \"population\": 273},\n",
    "    {\"country\": \"Brazil\", \"population\": 212},\n",
    "    {\"country\": \"Egypt\", \"population\": 102},\n",
    "    {\"country\": \"Spain\", \"population\": 47},\n",
    "    {\"country\": \"Ghana\", \"population\": 31},\n",
    "    {\"country\": \"Australia\", \"population\": 25},\n",
    "]\n",
    "\n",
    "create_1 = p | \"Create 1\" >> Create(elements_1)\n",
    "create_2 = p | \"Create 2\" >> Create(elements_2)\n",
    "\n",
    "# Left side of | has to be a tuple\n",
    "flattened = (create_1, create_2) | Flatten()\n",
    "\n",
    "ib.show_graph(p)\n",
    "ib.show(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Branching** The same way we can join two or more `PCollections` into one, we can use the same `PCollections` as input for one or more `PTransforms`/Sinks. It is as simple as referencing a previous section of a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "elements = [\n",
    "    {\"country\": \"China\", \"population\": 1389},\n",
    "    {\"country\": \"India\", \"population\": 1311},\n",
    "    {\"country\": \"USA\", \"population\": 331},\n",
    "    {\"country\": \"Ireland\", \"population\": 5}\n",
    "]\n",
    "\n",
    "create = p | \"Create\" >> Create(elements)\n",
    "\n",
    "country = create | \"country\" >> Map(lambda x: x[\"country\"])\n",
    "population = create | \"population\" >> Map(lambda x: x[\"population\"])\n",
    "\n",
    "ib.show_graph(p)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have two outputs, we need to use `ib.show` with both of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show(country, population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SUGGESTION*: In the previous example, try to modify it to use `ParDo` instead of `Map`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other more clever ways to branch. Sometimes, you want to split the `PCollection` into two or more `PCollections`:\n",
    "\n",
    "**`Partition`** sends elements to different `PTransforms` following a given function. It applies a function element-wise which outputs the index of the pipeline which the element should go to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "even, odd = (p | \"Create Numbers\" >> Create(range(10))\n",
    "               | \"Odd or Even\" >> Partition(lambda n, partitions: n % 2, 2))\n",
    "# lambda x,y: which partition fn, number partitions\n",
    "# even is when the fn outputs 0, odd when it outputs 1\n",
    "\n",
    "ib.show(even, odd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________\n",
    "This option still does not cover all possibilities. What if you wanted a particular element to fall into two or more different categories?\n",
    "\n",
    "We can use `ParDo` and the `TaggedOutput` option to achieve this. This is the most flexible way to branch `PCollections`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "class DifferentOutputsFn(beam.DoFn):\n",
    "    def process(self, element, x, y):\n",
    "        if element % x == 0:\n",
    "            yield pvalue.TaggedOutput(\"x\", element)\n",
    "\n",
    "        if element % y == 0:\n",
    "            yield pvalue.TaggedOutput(\"y\", element)\n",
    "\n",
    "        yield element\n",
    "\n",
    "\n",
    "diff_outputs = (p | \"Create\" >> Create(range(8))\n",
    "                  | \"Split Outputs\" >> ParDo(DifferentOutputsFn(), x=2, y=3).with_outputs(\"x\", \"y\"))\n",
    "\n",
    "multiple_x = diff_outputs.x\n",
    "multiple_y = diff_outputs.y\n",
    "all_outputs = diff_outputs[None] \n",
    "\n",
    "ib.show_graph(p)\n",
    "ib.show(multiple_x, multiple_y, all_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the code. \n",
    "\n",
    "We are sending the elements to three subsections:\n",
    "\n",
    "- Containing the multiples of X and/or Y: \n",
    "    * To send elements to those sections, use `yield` to that tagged output: `yield pvalue.TaggedOutput(\"x\", element)`\n",
    "    * To retrieve elements from that pipeline, simply refer to the original subpipeline `diff_outputs` with the tagged key *x* or *y*: `diff_outputs.x`\n",
    "\n",
    "\n",
    "- All elements: \n",
    "    * In this case, there is no need to specify the tagged output, so simply use `yield`\n",
    "    * To get this output, reference the elements without a tag by using `diff_outputs[None]`\n",
    "    \n",
    "Also, let's check how the `ParDo` is being used:\n",
    "\n",
    "- `ParDo(DifferentOutputsFn(), x=2, y=3).with_outputs(\"x\", \"y\"))`\n",
    "    * Use `with_outputs` to reference the name of the tagged outputs\n",
    "    * Note how the parameters `x` and `y` are passed to the `DifferentOutputsFn` Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at another example. In this case, the pipeline only process words with more than five letters and discard other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "class LengthStringFn(beam.DoFn):\n",
    "    def process(self, element, max_len):\n",
    "        if len(element) <= max_len:\n",
    "            yield pvalue.TaggedOutput(\"smaller\", element)\n",
    "        else:\n",
    "            yield element\n",
    "\n",
    "elements = [\"Beam\", \"Pipeline\", \"PCollection\", \"Map\", \"Notebook\"]\n",
    "\n",
    "string_length = (p | \"Create\" >> Create(elements)\n",
    "                   | \"Split Outputs\" >> ParDo(LengthStringFn(), max_len=5).with_outputs(\"smaller\"))\n",
    "\n",
    "smaller = string_length.smaller | \"Discarded\" >> Map(lambda e: logging.warning(f\"Discarded: {e}\"))\n",
    "\n",
    "bigger = string_length[None] | \"Filtered\" >> Map(lambda e: e.lower())\n",
    "\n",
    "ib.show_graph(p)\n",
    "ib.show(bigger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create elements (integers) coming from two sources, and categorize these elements as perfect squares or non-perfect squares. The output needs to be a tuple containing the original element and its square root. For example, for input `25`, the output needs to be `(25, 5.0)`.\n",
    "\n",
    "There are hints below and the solution at the end.\n",
    "\n",
    "Since we are going to test if the pipeline is right, be sure to name the final pipelines `perfect_squares` and `not_perfect_squares`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.testing.util import assert_that\n",
    "from apache_beam.testing.util import matches_all, equal_to\n",
    "from utils.solutions import solutions\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "def is_perfect_square(e):\n",
    "    boolean = e[1].is_integer()\n",
    "    return int(boolean)\n",
    "\n",
    "elements_1 = [1, 49]\n",
    "elements_2 = [3, 1024, 1729]\n",
    "\n",
    "# TODO: Finish the pipeline \n",
    "create_1 = p | \"Create1\" >> Create()\n",
    "\n",
    "ib.show_graph(p)\n",
    "ib.show(not_perfect_squares, perfect_squares)\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "# assert_that(pipeline, matcher, label(optional))\n",
    "assert_that(not_perfect_squares, equal_to(solutions[1][\"not_perfect_squares\"]), label=\"not_perfect_squares\")\n",
    "assert_that(perfect_square, equal_to(solutions[1][\"perfect_squares\"]), label=\"perfect_squares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create elements**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "    \n",
    "You need to use `Create` twice, since we are using two sources. Don't forget the name of the steps has to be different.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "   create_1 = p | \"Create_1\" >> Create(elements_1)\n",
    "   create_2 = p | \"Create_2\" >> Create(elements_2) \n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process created elements**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "    \n",
    "Since the pipeline reads from two sources and performs the same operations to them, you need to join them using a `Flatten`. To know if a number is a perfect square, you need to calculate its square root, you need a `Map`. Since the output requires a tuple, `Map` should generate it.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "flattened = ((create_1, create_2) | Flatten()\n",
    "                                  | \"Square root\" >> Map(lambda x: (x, math.sqrt(x))))\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split elements given according to a rule**\n",
    "<details><summary>Hint</summary>\n",
    "<p>\n",
    "\n",
    "You need to send elements to different steps given a rule (perfect square or not), you can do so using `Partition` or the general `ParDo withOutputTags`. Potentially, this could also be done with `Filter` (next notebook), but you need to process every element twice, so it's not recommended.\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```    \n",
    "not_perfect_squares, perfect_squares = flattened | \"Partition\" >> Partition(lambda x, partition: int(x[1].is_integer()), 2)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full code**\n",
    "<details><summary>Code</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "elements_1 = [1, 49]\n",
    "elements_2 = [3, 1024, 1729]\n",
    "\n",
    "# TODO: Finish the pipeline \n",
    "create_1 = p | \"Create1\" >> Create(elements_1)\n",
    "create_2 = p | \"Create2\" >> Create(elements_2) \n",
    "\n",
    "flattened = ((create_1, create_2) | Flatten()\n",
    "                                  | \"Square root\" >> Map(lambda x: (x, math.sqrt(x))))\n",
    "\n",
    "not_perfect_squares, perfect_squares = flattened | \"Partition\" >> Partition(lambda x, partition: int(x[1].is_integer()), 2)\n",
    "\n",
    "ib.show_graph(p)\n",
    "ib.show(not_perfect_squares, perfect_squares)\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "# assert_that(pipeline, matcher, label(optional))\n",
    "assert_that(not_perfect_squares, equal_to(solutions[1][\"not_perfect_squares\"]), label=\"not_perfect_squares\")\n",
    "assert_that(perfect_squares, equal_to(solutions[1][\"perfect_squares\"]), label=\"perfect_squares\")\n",
    "```\n",
    "    \n",
    "\n",
    "</p>\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.35.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.35.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
