{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O Operations\n",
    "\n",
    "In most cases, we don't use `Create` to create elements and print the elements as final destination, we use sources to read from and sinks to write to. They can be files in our system, buckets on Cloud Storage, BigQuery tables or Pub/Sub topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from utils.solutions import solutions\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam import  Map\n",
    "from apache_beam.transforms.combiners import Count\n",
    "from apache_beam.io.textio import ReadFromText, WriteToText\n",
    "\n",
    "from apache_beam.testing.util import assert_that\n",
    "from apache_beam.testing.util import matches_all, equal_to\n",
    "\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many [I/O operations](https://beam.apache.org/releases/pydoc/current/apache_beam.io.html). In this notebook we are only going to deal with reading/writing text files and we'll leave BigQuery and Pub/Sub for the next notebook ([Streaming](6_Streaming.ipynb)).\n",
    "\n",
    "**`ReadFromText`** reads from a file path. It can also be a Cloud Storage path. The parameter `min_bundle_size` sets the minimum [bundle](https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence) size of each split the source has (i.e., the file is split in N bundles of at least `min_bundle_size`). The parameter is optional and, if not set, Apache Beam will handle it for you.\n",
    "\n",
    "**`WriteToText`** writes to a file path or a Cloud Storage path. The optional parameter `num_shards` sets the number of output files. It is not recommended to change it, but certain use cases require a fixed amount of output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-4-bcb0ea0bd0f7>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-bcb0ea0bd0f7>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    | \"Map\" >> Map(print_fn)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "    \n",
    "path = \"input/example.txt\"\n",
    "output_path = \"output/example\"\n",
    "\n",
    "def print_fn(e):\n",
    "    # We are adding this step to know what the elements look like\n",
    "    print(\"Element: {}\".format(e))\n",
    "    return e\n",
    "\n",
    "write = (p | \"Read\" >> ReadFromText(path))\n",
    "           | \"Map\" >> Map(print_fn)\n",
    "           | \"Write\" >> WriteToText(file_path_prefix=output_path, file_name_suffix=\".txt\", num_shards=2))\n",
    "\n",
    "ib.show(write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case `ib.show()` gets the file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use wildcards as paths. Using the previous output as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "output_path = \"output/example\"\n",
    "\n",
    "read = p | \"Read\" >> ReadFromText(file_pattern=output_path + '*')\n",
    "\n",
    "ib.show_graph(p)\n",
    "ib.show(read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because the files are read in parallel, the order of the lines may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a public Cloud Storage bucket, there is the file `hamlet.txt` which the pipeline is using for this exercise.\n",
    "\n",
    "The goal is to know how many lines that file has. Optionally you can write the number to a file, but in the posted solution it won't be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())\n",
    "    \n",
    "path = \"gs://dataflow-samples/shakespeare/hamlet.txt\"\n",
    "\n",
    "# TODO: Finish the pipeline \n",
    "count_lines = (p | )\n",
    "\n",
    "ib.show(count_lines)\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "assert_that(count_lines, equal_to(solutions[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Read Hint</summary>\n",
    "<p>\n",
    "When the file is read, it's already split by lines.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details><summary><b>Code</b></summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "p = beam.Pipeline(InteractiveRunner())\n",
    "    \n",
    "path = \"gs://dataflow-samples/shakespeare/hamlet.txt\"\n",
    "\n",
    "count_lines = (p | ReadFromText(path)\n",
    "                 | Count.Globally())\n",
    "\n",
    "ib.show(count_lines)\n",
    "\n",
    "# For testing the solution - Don't modify\n",
    "assert_that(count_lines, equal_to(solutions[5]))\n",
    "```\n",
    "</p>\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.35.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.35.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
